{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jauharmz/Orca_Files/blob/main/ORCA_Test_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Importing Libraries"
      ],
      "metadata": {
        "id": "9PcY58lOjtx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "\n",
        "# Install a Miniforge version that ships with Python 3.12 to match Colab\n",
        "condacolab.install_from_url(\"https://github.com/conda-forge/miniforge/releases/download/24.9.0-0/Miniforge3-24.9.0-0-Linux-x86_64.sh\")"
      ],
      "metadata": {
        "id": "zHI9NO3_gPXH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESTART RUNTIME"
      ],
      "metadata": {
        "id": "8g-1_zFK90ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import condacolab\n",
        "condacolab.check()\n",
        "\n",
        "# Install RDKit into the existing base environment\n",
        "!mamba install -c conda-forge rdkit openbabel -y\n",
        "!pip install py3Dmol\n",
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "HNA-wxCogT-n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIkB-AnXtZYg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from openbabel import pybel\n",
        "from tempfile import NamedTemporaryFile\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, snapshot_download\n",
        "\n",
        "API_KEY = ''\n",
        "\n",
        "login(API_KEY)"
      ],
      "metadata": {
        "id": "zaKFEaM2knD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Define Parsing Functions"
      ],
      "metadata": {
        "id": "7W4dnRwXjzzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ---------- Helper Function for Scientific Notation ----------\n",
        "def parse_float(value):\n",
        "    \"\"\"Convert a string to float, handling scientific notation.\"\"\"\n",
        "    try:\n",
        "        return float(value)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# ---------- SMILES Generation Function ----------\n",
        "def coords_to_smiles(mol_coords):\n",
        "    \"\"\"\n",
        "    Convert (symbol, x, y, z) coordinates into a SMILES string using OpenBabel (pybel).\n",
        "    \"\"\"\n",
        "    if not mol_coords:\n",
        "        return None\n",
        "\n",
        "    # Write coords to a temporary XYZ file\n",
        "    with NamedTemporaryFile(\"w\", suffix=\".xyz\", delete=False) as tmp:\n",
        "        tmp.write(f\"{len(mol_coords)}\\n\")\n",
        "        tmp.write(\"Generated from dataframe\\n\")\n",
        "        for atom, x, y, z in mol_coords:\n",
        "            tmp.write(f\"{atom} {x:.6f} {y:.6f} {z:.6f}\\n\")\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    try:\n",
        "        # Read with pybel\n",
        "        mol = next(pybel.readfile(\"xyz\", tmp_path))\n",
        "\n",
        "        # Convert to SMILES\n",
        "        smi = mol.write(\"smi\").strip()\n",
        "        return smi.split()[0]\n",
        "    except Exception as e:\n",
        "        return None\n",
        "    finally:\n",
        "        # Clean up temporary file\n",
        "        try:\n",
        "            os.unlink(tmp_path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# ---------- Geometry Info ----------\n",
        "def parse_geometry_info(text):\n",
        "    \"\"\"Parse geometry file name, charge, and multiplicity from ORCA output.\"\"\"\n",
        "    geometry_info = {\"filename\": None, \"charge\": None, \"multiplicity\": None}\n",
        "\n",
        "    # Try to find geometry file from \"The coordinates will be read from file\" line\n",
        "    coords_match = re.search(\n",
        "        r\"The coordinates will be read from file:\\s*(\\S+)\",\n",
        "        text\n",
        "    )\n",
        "    if coords_match:\n",
        "        geometry_file = coords_match.group(1)\n",
        "        # Strip extension to get base name (e.g., \"phs0\" from \"phs0.xyz\")\n",
        "        geometry_info[\"filename\"] = os.path.splitext(geometry_file)[0]\n",
        "\n",
        "    # Try to find geometry file, charge, and multiplicity from input section\n",
        "    input_section_match = re.search(\n",
        "        r\"={10,}\\s*(INPUT FILE|INPUT)\\s*\\n={10,}\\n(.*?)\\*{4}END OF INPUT\\*{4}\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "    if input_section_match:\n",
        "        input_section = input_section_match.group(2)\n",
        "        # Look for * xyzfile charge multiplicity filename\n",
        "        xyzfile_match = re.search(\n",
        "            r\"\\*\\s*xyzfile\\s+(\\d+)\\s+(\\d+)\\s+(\\S+)\",\n",
        "            input_section, flags=re.I\n",
        "        )\n",
        "        if xyzfile_match:\n",
        "            geometry_info[\"charge\"] = int(xyzfile_match.group(1))\n",
        "            geometry_info[\"multiplicity\"] = int(xyzfile_match.group(2))\n",
        "            geometry_file = xyzfile_match.group(3)\n",
        "            # Strip extension to get base name\n",
        "            geometry_info[\"filename\"] = os.path.splitext(geometry_file)[0]\n",
        "\n",
        "    # If filename is still None, fallback to using the input filename without extension\n",
        "    if geometry_info[\"filename\"] is None:\n",
        "        return None\n",
        "\n",
        "    return geometry_info\n",
        "\n",
        "# ---------- Cartesian ----------\n",
        "def parse_last_cartesian(text):\n",
        "    \"\"\"Parse the last Cartesian coordinates block from ORCA output.\"\"\"\n",
        "    blocks = re.findall(\n",
        "        r\"CARTESIAN COORDINATES \\(ANGSTROEM\\)\\n-+\\n(.*?)\\n\\n\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not blocks:\n",
        "        return None\n",
        "    last_block = blocks[-1].strip().splitlines()\n",
        "    coords = []\n",
        "    for line in last_block:\n",
        "        parts = line.split()\n",
        "        if len(parts) < 4:\n",
        "            continue\n",
        "        atom = parts[0]\n",
        "        x, y, z = map(parse_float, parts[1:4])\n",
        "        if None in (x, y, z):\n",
        "            continue\n",
        "        coords.append((atom, x, y, z))\n",
        "    return coords\n",
        "\n",
        "# ---------- Internal ----------\n",
        "def parse_last_internal(text):\n",
        "    \"\"\"Parse the last internal coordinates block from ORCA output.\"\"\"\n",
        "    blocks = re.findall(\n",
        "        r\"INTERNAL COORDINATES \\(ANGSTROEM\\)\\n-+\\n(.*?)\\n\\n\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not blocks:\n",
        "        return None\n",
        "    last_block = blocks[-1].strip().splitlines()\n",
        "    coords = []\n",
        "    for line in last_block:\n",
        "        parts = line.split()\n",
        "        if len(parts) < 7:\n",
        "            continue\n",
        "        atom = parts[0]\n",
        "        bond, angle, dihedral = map(parse_float, parts[4:7])\n",
        "        if None in (bond, angle, dihedral):\n",
        "            continue\n",
        "        coords.append((atom, bond, angle, dihedral))\n",
        "    return coords\n",
        "\n",
        "# ---------- Orbitals ----------\n",
        "def parse_last_orbitals(text, as_df=False):\n",
        "    \"\"\"Parse the last orbital energies block from ORCA output.\n",
        "    Handles both closed-shell and open-shell (spin-polarized) cases.\n",
        "    \"\"\"\n",
        "    # --- Case 1: Open-shell (spin up / spin down blocks) ---\n",
        "    spin_blocks = re.findall(\n",
        "        r\"SPIN UP ORBITALS\\n.*?\\n(.*?)(?=\\n\\s*SPIN DOWN ORBITALS|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    spin_down_blocks = re.findall(\n",
        "        r\"SPIN DOWN ORBITALS\\n.*?\\n(.*?)(?:\\n\\n|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "\n",
        "    if spin_blocks and spin_down_blocks:\n",
        "        spins = [(\"up\", spin_blocks[-1]), (\"down\", spin_down_blocks[-1])]\n",
        "    else:\n",
        "        # --- Case 2: Closed-shell (single ORBITAL ENERGIES block) ---\n",
        "        blocks = re.findall(\n",
        "            r\"ORBITAL ENERGIES\\n-+\\n\\s*NO.*?\\n(.*?)(?:\\n\\n|\\Z)\",\n",
        "            text, flags=re.S\n",
        "        )\n",
        "        if not blocks:\n",
        "            return None\n",
        "        spins = [(\"na\", blocks[-1])]  # \"na\" = not applicable (closed shell)\n",
        "\n",
        "    orbitals = []\n",
        "    for spin, block in spins:\n",
        "        lines = block.strip().splitlines()\n",
        "        for line in lines:\n",
        "            parts = line.split()\n",
        "            if not parts or not parts[0].isdigit() or len(parts) < 4:\n",
        "                continue\n",
        "\n",
        "            occ = parse_float(parts[1])\n",
        "            energy_h = parse_float(parts[2])\n",
        "            energy_ev = parse_float(parts[3])\n",
        "\n",
        "            if None in (occ, energy_h, energy_ev):\n",
        "                continue\n",
        "\n",
        "            # Filter out fake test data - ORCA only produces OCC values of 0.0, 1.0, or 2.0\n",
        "            if occ not in [0.0, 1.0, 2.0]:\n",
        "                continue\n",
        "\n",
        "            # Additional check for obvious fake data patterns\n",
        "            if energy_h == occ + 1.0 and energy_ev == occ + 2.0:\n",
        "                continue\n",
        "\n",
        "            orbitals.append({\n",
        "                \"OCC\": occ,\n",
        "                \"Eh\": energy_h,\n",
        "                \"eV\": energy_ev,\n",
        "                \"spin\": spin\n",
        "            })\n",
        "\n",
        "    if not orbitals:\n",
        "        return None\n",
        "\n",
        "    # Separate occupied and virtual orbitals\n",
        "    occupied = [orb for orb in orbitals if orb[\"OCC\"] > 0.0]\n",
        "    virtual = [orb for orb in orbitals if orb[\"OCC\"] == 0.0]\n",
        "\n",
        "    # Sort occupied orbitals by energy (descending - highest energy first)\n",
        "    # HOMO is the highest energy occupied orbital\n",
        "    occupied.sort(key=lambda x: x[\"Eh\"], reverse=True)\n",
        "\n",
        "    # Sort virtual orbitals by energy (ascending - lowest energy first)\n",
        "    # LUMO is the lowest energy virtual orbital\n",
        "    virtual.sort(key=lambda x: x[\"Eh\"])\n",
        "\n",
        "    # Assign levels for occupied orbitals\n",
        "    for i, orb in enumerate(occupied):\n",
        "        orb[\"lvl\"] = i  # HOMO = 0, HOMO-1 = 1, HOMO-2 = 2, etc.\n",
        "\n",
        "    # Assign levels for virtual orbitals\n",
        "    for i, orb in enumerate(virtual):\n",
        "        orb[\"lvl\"] = i  # LUMO = 0, LUMO+1 = 1, LUMO+2 = 2, etc.\n",
        "\n",
        "    # Combine back into original order (by energy)\n",
        "    all_orbitals = occupied + virtual\n",
        "    all_orbitals.sort(key=lambda x: x[\"Eh\"])\n",
        "\n",
        "    if as_df:\n",
        "        return pd.DataFrame(all_orbitals)\n",
        "\n",
        "    return all_orbitals\n",
        "\n",
        "# ---------- Vibrations ----------\n",
        "def parse_last_vibrations(text):\n",
        "    blocks = re.findall(\n",
        "        r\"VIBRATIONAL FREQUENCIES\\n-+\\n.*?\\n\\n(.*?)(?:\\n\\n|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not blocks:\n",
        "        return None\n",
        "\n",
        "    last_block = blocks[-1].strip().splitlines()\n",
        "    vib_data = []\n",
        "    for line in last_block:\n",
        "        parts = line.split()\n",
        "        if not parts or not parts[0].rstrip(\":\").isdigit():\n",
        "            continue\n",
        "        idx = int(parts[0].rstrip(\":\"))\n",
        "        freq = float(parts[1])\n",
        "\n",
        "        # check imaginary\n",
        "        img = 1 if freq < 0 else 0\n",
        "\n",
        "        vib_data.append({\n",
        "            \"freq_cm-1\": freq,\n",
        "            \"img\": img\n",
        "        })\n",
        "\n",
        "    return vib_data\n",
        "\n",
        "# ---------- IR Spectrum ----------\n",
        "def parse_ir_spectrum(text):\n",
        "    match = re.search(\n",
        "        r\"-{5,}\\nIR SPECTRUM\\n-+\\n.*?\\n-+\\n(.*?)(?:\\n\\n|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    block = match.group(1).strip().splitlines()\n",
        "    ir_data = []\n",
        "    for line in block:\n",
        "        parts = line.split()\n",
        "        if not parts or not parts[0].rstrip(\":\").isdigit():\n",
        "            continue\n",
        "\n",
        "        idx = int(parts[0].rstrip(\":\"))\n",
        "        freq = float(parts[1])\n",
        "        eps = float(parts[2])\n",
        "        intensity = float(parts[3])\n",
        "        t2 = float(parts[4])\n",
        "\n",
        "        ir_data.append({\n",
        "            \"freq_cm-1\": freq,\n",
        "            \"eps\": eps,\n",
        "            \"intensity_km/mol\": intensity,\n",
        "            \"t2\": t2\n",
        "        })\n",
        "\n",
        "    return ir_data\n",
        "\n",
        "# ---------- Raman Spectrum ----------\n",
        "def parse_raman_spectrum(text, as_df=True):\n",
        "    match = re.search(\n",
        "        r\"RAMAN SPECTRUM\\s*-+\\s*Mode.*?-+\\s*(.*?)(?:\\n\\n|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    block = match.group(1).strip().splitlines()\n",
        "    raman_data = []\n",
        "    for line in block:\n",
        "        parts = line.split()\n",
        "        if len(parts) < 4:\n",
        "            continue\n",
        "\n",
        "        freq = float(parts[1])\n",
        "        activity = float(parts[2])\n",
        "        depol = float(parts[3])\n",
        "\n",
        "        raman_data.append({\n",
        "            \"freq_cm-1\": freq,\n",
        "            \"activity\": activity,\n",
        "            \"depolarization\": depol\n",
        "        })\n",
        "\n",
        "    if as_df:\n",
        "        return pd.DataFrame(raman_data)\n",
        "    return raman_data\n",
        "\n",
        "# ---------- Gibbs Energy ----------\n",
        "def parse_gibbs_energy(text):\n",
        "    match = re.search(\n",
        "        r\"Final Gibbs free energy\\s+\\.{3,}\\s+(-?\\d+\\.\\d+)\\s+Eh\",\n",
        "        text\n",
        "    )\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return None\n",
        "\n",
        "# ---------- Single Point Energy ----------\n",
        "def parse_single_point_energy(text):\n",
        "    matches = re.findall(r\"FINAL SINGLE POINT ENERGY\\s+(-?\\d+\\.\\d+)\", text)\n",
        "    if matches:\n",
        "        return float(matches[-1])  # last match = optimized\n",
        "    return None\n",
        "\n",
        "# ---------- TDDFT States ----------\n",
        "def parse_tddft_states(text, orbitals=None):\n",
        "    states = []\n",
        "    state_block_pattern = re.compile(\n",
        "        r\"STATE\\s+(\\d+):\\s+E=\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+au\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+eV\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+cm\\*{1,2}-1([\\s\\S]+?)(?=STATE|\\Z)\"\n",
        "    )\n",
        "    transition_pattern = re.compile(\n",
        "        r\"(\\d+)a\\s*->\\s*(\\d+)a\\s*:\\s*([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s*\\(c=\\s*([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\)\"\n",
        "    )\n",
        "\n",
        "    from_orbs = set()\n",
        "    to_orbs = set()\n",
        "    for state_match in state_block_pattern.finditer(text):\n",
        "        for trans_match in transition_pattern.finditer(state_match.group(5)):\n",
        "            from_orbs.add(int(trans_match.group(1)))\n",
        "            to_orbs.add(int(trans_match.group(2)))\n",
        "\n",
        "    homo_index = max(from_orbs) if from_orbs else None\n",
        "    lumo_index = min(to_orbs) if to_orbs else None\n",
        "\n",
        "    if (homo_index is None or lumo_index is None) and orbitals:\n",
        "        try:\n",
        "            max_occupied = max(i for i, o in enumerate(orbitals) if o[\"OCC\"] > 0.0)\n",
        "            if homo_index is None:\n",
        "                homo_index = max_occupied + 1  # ORCA indices are 1-based\n",
        "            if lumo_index is None:\n",
        "                lumo_index = max_occupied + 2\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    if homo_index is None or lumo_index is None:\n",
        "        homo_index, lumo_index = float('inf'), float('inf')\n",
        "\n",
        "    for state_match in state_block_pattern.finditer(text):\n",
        "        state_num = int(state_match.group(1))\n",
        "        energy_au = parse_float(state_match.group(2))\n",
        "        energy_ev = parse_float(state_match.group(3))\n",
        "        energy_cm1 = parse_float(state_match.group(4))\n",
        "        transitions_block = state_match.group(5)\n",
        "\n",
        "        for trans_match in transition_pattern.finditer(transitions_block):\n",
        "            from_orb = int(trans_match.group(1))\n",
        "            to_orb = int(trans_match.group(2))\n",
        "            weight = parse_float(trans_match.group(3))\n",
        "            coeff = parse_float(trans_match.group(4))\n",
        "\n",
        "            homo = None\n",
        "            lumo = None\n",
        "            if from_orb <= homo_index:\n",
        "                homo = from_orb - homo_index\n",
        "            if to_orb >= lumo_index:\n",
        "                lumo = to_orb - lumo_index\n",
        "\n",
        "            states.append({\n",
        "                \"state\": state_num,\n",
        "                \"energy_au\": energy_au,\n",
        "                \"energy_ev\": energy_ev,\n",
        "                \"energy_cm1\": energy_cm1,\n",
        "                \"homo\": homo,\n",
        "                \"lumo\": lumo,\n",
        "                \"weight\": weight,\n",
        "                \"coeff\": coeff,\n",
        "                \"from_orb\": from_orb,\n",
        "                \"to_orb\": to_orb\n",
        "            })\n",
        "\n",
        "    df_states = pd.DataFrame(states) if states else pd.DataFrame()\n",
        "    return df_states\n",
        "\n",
        "# ---------- Electric Dipole Spectrum ----------\n",
        "def parse_electric_dipole_spectrum(text, as_df=True):\n",
        "    \"\"\"Parse both regular and SOC-corrected absorption spectra via transition electric dipole moments from ORCA output.\"\"\"\n",
        "    dipole_data = {\"abs\": [], \"soc\": []}\n",
        "\n",
        "    # Pattern for regular absorption spectrum\n",
        "    abs_match = re.search(\n",
        "        r\"-+\\n\\s*ABSORPTION SPECTRUM VIA TRANSITION ELECTRIC DIPOLE MOMENTS\\s*\\n-+\\n.*?Transition.*?\\n-+\\n(.*?)(?:\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "\n",
        "    # Pattern for SOC-corrected absorption spectrum\n",
        "    soc_match = re.search(\n",
        "        r\"-+\\n\\s*SOC CORRECTED ABSORPTION SPECTRUM VIA TRANSITION ELECTRIC DIPOLE MOMENTS\\s*\\n-+\\n.*?Transition.*?\\n-+\\n(.*?)(?:\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "\n",
        "    line_pattern = re.compile(\n",
        "        r\"^\\s*(\\S+\\s*->\\s*\\S+)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s*$\",\n",
        "        re.M\n",
        "    )\n",
        "\n",
        "    # Parse regular absorption spectrum\n",
        "    if abs_match:\n",
        "        for line in abs_match.group(1).strip().splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            match_line = line_pattern.match(line)\n",
        "            if not match_line:\n",
        "                continue\n",
        "\n",
        "            transition = match_line.group(1).strip()\n",
        "            energy_ev = parse_float(match_line.group(2))\n",
        "            energy_cm1 = parse_float(match_line.group(3))\n",
        "            wavelength_nm = parse_float(match_line.group(4))\n",
        "            fosc_d2 = parse_float(match_line.group(5))\n",
        "            d2 = parse_float(match_line.group(6))\n",
        "            dx = parse_float(match_line.group(7))\n",
        "            dy = parse_float(match_line.group(8))\n",
        "            dz = parse_float(match_line.group(9))\n",
        "\n",
        "            dipole_data[\"abs\"].append({\n",
        "                \"transition\": transition,\n",
        "                \"energy_ev\": energy_ev,\n",
        "                \"energy_cm1\": energy_cm1,\n",
        "                \"wavelength_nm\": wavelength_nm,\n",
        "                \"fosc_d2\": fosc_d2,\n",
        "                \"d2\": d2,\n",
        "                \"dx\": dx,\n",
        "                \"dy\": dy,\n",
        "                \"dz\": dz\n",
        "            })\n",
        "\n",
        "    # Parse SOC-corrected absorption spectrum\n",
        "    if soc_match:\n",
        "        for line in soc_match.group(1).strip().splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            match_line = line_pattern.match(line)\n",
        "            if not match_line:\n",
        "                continue\n",
        "\n",
        "            transition = match_line.group(1).strip()\n",
        "            energy_ev = parse_float(match_line.group(2))\n",
        "            energy_cm1 = parse_float(match_line.group(3))\n",
        "            wavelength_nm = parse_float(match_line.group(4))\n",
        "            fosc_d2 = parse_float(match_line.group(5))\n",
        "            d2 = parse_float(match_line.group(6))\n",
        "            dx = parse_float(match_line.group(7))\n",
        "            dy = parse_float(match_line.group(8))\n",
        "            dz = parse_float(match_line.group(9))\n",
        "\n",
        "            dipole_data[\"soc\"].append({\n",
        "                \"transition\": transition,\n",
        "                \"energy_ev\": energy_ev,\n",
        "                \"energy_cm1\": energy_cm1,\n",
        "                \"wavelength_nm\": wavelength_nm,\n",
        "                \"fosc_d2\": fosc_d2,\n",
        "                \"d2\": d2,\n",
        "                \"dx\": dx,\n",
        "                \"dy\": dy,\n",
        "                \"dz\": dz\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame if as_df is True\n",
        "    if as_df:\n",
        "        dipole_data[\"abs\"] = pd.DataFrame(dipole_data[\"abs\"]) if dipole_data[\"abs\"] else pd.DataFrame()\n",
        "        dipole_data[\"soc\"] = pd.DataFrame(dipole_data[\"soc\"]) if dipole_data[\"soc\"] else pd.DataFrame()\n",
        "\n",
        "    return dipole_data\n",
        "\n",
        "# ---------- Velocity Dipole Spectrum ----------\n",
        "def parse_velocity_dipole_spectrum(text, as_df=True):\n",
        "    \"\"\"Parse both regular and SOC-corrected absorption spectra via transition velocity dipole moments from ORCA output.\"\"\"\n",
        "    dipole_data = {\"abs\": [], \"soc\": []}\n",
        "\n",
        "    # Pattern for regular absorption spectrum\n",
        "    abs_match = re.search(\n",
        "        r\"-+\\n\\s*ABSORPTION SPECTRUM VIA TRANSITION VELOCITY DIPOLE MOMENTS\\s*\\n-+\\n.*?Transition.*?\\n-+\\n(.*?)(?:\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "\n",
        "    # Pattern for SOC-corrected absorption spectrum\n",
        "    soc_match = re.search(\n",
        "        r\"-+\\n\\s*SOC CORRECTED ABSORPTION SPECTRUM VIA TRANSITION VELOCITY DIPOLE MOMENTS\\s*\\n-+\\n.*?Transition.*?\\n-+\\n(.*?)(?:\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "\n",
        "    line_pattern = re.compile(\n",
        "        r\"^\\s*(\\S+\\s*->\\s*\\S+)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s+([-]?\\d*\\.?\\d+[Ee]?[+-]?\\d*)\\s*$\",\n",
        "        re.M\n",
        "    )\n",
        "\n",
        "    # Parse regular absorption spectrum\n",
        "    if abs_match:\n",
        "        for line in abs_match.group(1).strip().splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            match_line = line_pattern.match(line)\n",
        "            if not match_line:\n",
        "                continue\n",
        "\n",
        "            transition = match_line.group(1).strip()\n",
        "            energy_ev = parse_float(match_line.group(2))\n",
        "            energy_cm1 = parse_float(match_line.group(3))\n",
        "            wavelength_nm = parse_float(match_line.group(4))\n",
        "            fosc_p2 = parse_float(match_line.group(5))\n",
        "            p2 = parse_float(match_line.group(6))\n",
        "            px = parse_float(match_line.group(7))\n",
        "            py = parse_float(match_line.group(8))\n",
        "            pz = parse_float(match_line.group(9))\n",
        "\n",
        "            dipole_data[\"abs\"].append({\n",
        "                \"transition\": transition,\n",
        "                \"energy_ev\": energy_ev,\n",
        "                \"energy_cm1\": energy_cm1,\n",
        "                \"wavelength_nm\": wavelength_nm,\n",
        "                \"fosc_p2\": fosc_p2,\n",
        "                \"p2\": p2,\n",
        "                \"px\": px,\n",
        "                \"py\": py,\n",
        "                \"pz\": pz\n",
        "            })\n",
        "\n",
        "    # Parse SOC-corrected absorption spectrum\n",
        "    if soc_match:\n",
        "        for line in soc_match.group(1).strip().splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            match_line = line_pattern.match(line)\n",
        "            if not match_line:\n",
        "                continue\n",
        "\n",
        "            transition = match_line.group(1).strip()\n",
        "            energy_ev = parse_float(match_line.group(2))\n",
        "            energy_cm1 = parse_float(match_line.group(3))\n",
        "            wavelength_nm = parse_float(match_line.group(4))\n",
        "            fosc_p2 = parse_float(match_line.group(5))\n",
        "            p2 = parse_float(match_line.group(6))\n",
        "            px = parse_float(match_line.group(7))\n",
        "            py = parse_float(match_line.group(8))\n",
        "            pz = parse_float(match_line.group(9))\n",
        "\n",
        "            dipole_data[\"soc\"].append({\n",
        "                \"transition\": transition,\n",
        "                \"energy_ev\": energy_ev,\n",
        "                \"energy_cm1\": energy_cm1,\n",
        "                \"wavelength_nm\": wavelength_nm,\n",
        "                \"fosc_p2\": fosc_p2,\n",
        "                \"p2\": p2,\n",
        "                \"px\": px,\n",
        "                \"py\": py,\n",
        "                \"pz\": pz\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame if as_df is True\n",
        "    if as_df:\n",
        "        dipole_data[\"abs\"] = pd.DataFrame(dipole_data[\"abs\"]) if dipole_data[\"abs\"] else pd.DataFrame()\n",
        "        dipole_data[\"soc\"] = pd.DataFrame(dipole_data[\"soc\"]) if dipole_data[\"soc\"] else pd.DataFrame()\n",
        "\n",
        "    return dipole_data\n",
        "\n",
        "# ---------- Spectrum File Parser ----------\n",
        "def parse_spectrum_file(filename, spectrum_type=None, as_df=True):\n",
        "    \"\"\"Parse a .spectrum file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to .spectrum file\n",
        "        spectrum_type (str): Type of spectrum (AH, AHAS, VG, FLUOR, PHOSP)\n",
        "        as_df (bool): Return DataFrame if True\n",
        "\n",
        "    Returns:\n",
        "        DataFrame or list of dict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filename, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "    except FileNotFoundError:\n",
        "        return pd.DataFrame() if as_df else []\n",
        "    except Exception:\n",
        "        return pd.DataFrame() if as_df else []\n",
        "\n",
        "    spectrum_data = []\n",
        "    for line in lines[1:]:  # Skip header\n",
        "        parts = line.strip().split()\n",
        "        if not parts:\n",
        "            continue\n",
        "\n",
        "        # Common energy column\n",
        "        energy = parse_float(parts[0])\n",
        "\n",
        "        if spectrum_type in (\"AH\", \"AHAS\", \"VG\"):  # Absorption\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            total_spectrum = parse_float(parts[1])\n",
        "            intensity_fc = parse_float(parts[2])\n",
        "            intensity_ht = parse_float(parts[3])\n",
        "            if None in (energy, total_spectrum, intensity_fc, intensity_ht):\n",
        "                continue\n",
        "            spectrum_data.append({\n",
        "                \"energy_cm1\": energy,\n",
        "                \"total_spectrum\": total_spectrum,\n",
        "                \"intensity_fc\": intensity_fc,\n",
        "                \"intensity_ht\": intensity_ht\n",
        "            })\n",
        "\n",
        "        elif spectrum_type in (\"FLUOR\", \"PHOSP\"):  # Emission\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            total_spectrum = parse_float(parts[1])\n",
        "            if None in (energy, total_spectrum):\n",
        "                continue\n",
        "            spectrum_data.append({\n",
        "                \"energy_cm1\": energy,\n",
        "                \"total_spectrum\": total_spectrum,\n",
        "                \"intensity_emission\": total_spectrum\n",
        "            })\n",
        "\n",
        "    result = pd.DataFrame(spectrum_data) if as_df else spectrum_data\n",
        "    return result\n",
        "\n",
        "# ---------- ESD Flag Parser ----------\n",
        "def parse_esd_flag(text):\n",
        "    \"\"\"Parse %ESD block in ORCA output (Absorption HESSFLAG or Emission ESDFlag).\"\"\"\n",
        "    input_section_match = re.search(\n",
        "        r\"={10,}\\s*INPUT FILE\\s*\\n={10,}\\n(.*?)\\*{4}END OF INPUT\\*{4}\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if not input_section_match:\n",
        "        return None\n",
        "\n",
        "    input_section = input_section_match.group(1)\n",
        "\n",
        "    # First check for ESDFlag (emission)\n",
        "    match_emission = re.search(r\"ESDFlag\\s+(\\w+)\", input_section, flags=re.S | re.I)\n",
        "    if match_emission:\n",
        "        return match_emission.group(1).upper()\n",
        "\n",
        "    # Fallback: check for HESSFLAG (absorption)\n",
        "    match_absorption = re.search(r\"HESSFLAG\\s+(\\w+)\", input_section, flags=re.S | re.I)\n",
        "    if match_absorption:\n",
        "        return match_absorption.group(1).upper()\n",
        "\n",
        "    return None\n",
        "\n",
        "# ---------- NMR Data Parser ----------\n",
        "def parse_nmr_data(text, as_df=True):\n",
        "    nmr_data = {\"shielding\": [], \"coupling\": []}\n",
        "\n",
        "    # Parse shielding\n",
        "    shielding_match = re.search(\n",
        "        r\"CHEMICAL SHIELDING SUMMARY \\(ppm\\)\\s*-+\\s*Nucleus\\s+Element\\s+Isotropic\\s+Anisotropy\\s*-+\\s*([\\s\\S]*?)(?=\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if shielding_match:\n",
        "        lines = shielding_match.group(1).strip().split('\\n')\n",
        "        for line in lines:\n",
        "            parts = line.split()\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            nucleus = parts[0]\n",
        "            element = parts[1]\n",
        "            isotropic = parse_float(parts[2])\n",
        "            anisotropy = parse_float(parts[3])\n",
        "            if None in (isotropic, anisotropy):\n",
        "                continue\n",
        "            nmr_data[\"shielding\"].append({\n",
        "                \"Nucleus\": nucleus,\n",
        "                \"Element\": element,\n",
        "                \"Isotropic\": isotropic,\n",
        "                \"Anisotropy\": anisotropy\n",
        "            })\n",
        "\n",
        "    # Parse coupling\n",
        "    coupling_match = re.search(\n",
        "        r\"SUMMARY OF ISOTROPIC COUPLING CONSTANTS\\s*J\\s*\\(Hz\\)\\s*-+\\s*([\\s\\S]*?)(?=\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S\n",
        "    )\n",
        "    if coupling_match:\n",
        "        lines = coupling_match.group(1).strip().split('\\n')\n",
        "        if not lines:\n",
        "            return nmr_data\n",
        "\n",
        "        # Find header line (contains nucleus labels)\n",
        "        header = None\n",
        "        data_start = 0\n",
        "        for i, line in enumerate(lines):\n",
        "            parts = line.split()\n",
        "            if parts and ('H' in parts or 'C' in parts or 'N' in parts or 'O' in parts):\n",
        "                header = parts\n",
        "                data_start = i + 1\n",
        "                break\n",
        "\n",
        "        if header:\n",
        "            # Process data lines\n",
        "            for line in lines[data_start:]:\n",
        "                parts = line.split()\n",
        "                if len(parts) < len(header) + 2:\n",
        "                    continue\n",
        "                nucleus1 = f\"{parts[0]} {parts[1]}\"  # e.g., \"7 H\"\n",
        "                for i, nucleus2 in enumerate(header):\n",
        "                    j_hz = parse_float(parts[i + 2])\n",
        "                    if j_hz is None:\n",
        "                        continue\n",
        "                    if abs(j_hz) > 1e-6:  # Only include non-zero couplings\n",
        "                        nmr_data[\"coupling\"].append({\n",
        "                            \"Nucleus1\": nucleus1,\n",
        "                            \"Nucleus2\": nucleus2,\n",
        "                            \"J_Hz\": j_hz\n",
        "                        })\n",
        "\n",
        "    # Convert to DataFrame if as_df is True\n",
        "    if as_df:\n",
        "        nmr_data[\"shielding\"] = pd.DataFrame(nmr_data[\"shielding\"]) if nmr_data[\"shielding\"] else pd.DataFrame()\n",
        "        nmr_data[\"coupling\"] = pd.DataFrame(nmr_data[\"coupling\"]) if nmr_data[\"coupling\"] else pd.DataFrame()\n",
        "\n",
        "    return nmr_data\n",
        "\n",
        "# ---------- Mulliken Charges Parser ----------\n",
        "def parse_mulliken_charges(text, as_df=True):\n",
        "    \"\"\"Parse Mulliken population analysis (atomic charges) from ORCA output.\"\"\"\n",
        "    charges_data = []\n",
        "\n",
        "    match = re.search(\n",
        "        r\"-+\\n\\s*MULLIKEN POPULATION ANALYSIS\\s*\\n-+\\n\\s*#\\s*Atom\\s*Element\\s*Pop\\s*Charge.*?\\n\\s*-+\\n(.*?)(?:\\n{2,}|\\Z)\",\n",
        "        text, flags=re.S | re.I\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        lines = match.group(1).strip().split('\\n')\n",
        "        for line in lines:\n",
        "            parts = re.split(r'\\s+', line.strip())\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            nucleus = parts[0]\n",
        "            element = parts[1]\n",
        "            pop = parse_float(parts[2])\n",
        "            charge = parse_float(parts[3])\n",
        "            if None in (pop, charge):\n",
        "                continue\n",
        "            charges_data.append({\n",
        "                \"Nucleus\": nucleus,\n",
        "                \"Element\": element,\n",
        "                \"Population\": pop,\n",
        "                \"Charge\": charge\n",
        "            })\n",
        "\n",
        "    if as_df:\n",
        "        return pd.DataFrame(charges_data) if charges_data else pd.DataFrame()\n",
        "    return charges_data\n",
        "\n",
        "def parse_internal(text, as_df=True):\n",
        "    \"\"\"Parse optimized bond lengths, angles, and dihedrals from ORCA output.\"\"\"\n",
        "    internal_data = {\"bonds\": [], \"angles\": [], \"dihedrals\": []}\n",
        "\n",
        "    # Split text into lines and find the start of internal coordinates data\n",
        "    lines = text.split('\\n')\n",
        "    start_idx = None\n",
        "    end_idx = None\n",
        "\n",
        "    # Find the header line with \"Definition\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'Definition' in line and 'OldVal' in line and 'dE/dq' in line and 'FinalVal' in line:\n",
        "            start_idx = i + 2  # Skip the header and separator line\n",
        "            break\n",
        "\n",
        "    if start_idx is None:\n",
        "        if as_df:\n",
        "            return {\n",
        "                \"bonds\": pd.DataFrame(),\n",
        "                \"angles\": pd.DataFrame(),\n",
        "                \"dihedrals\": pd.DataFrame()\n",
        "            }\n",
        "        return internal_data\n",
        "\n",
        "    # Find the end of the data (next dashed line or end of file)\n",
        "    for i in range(start_idx, len(lines)):\n",
        "        line = lines[i].strip()\n",
        "        if line.startswith('---') or (line == '' and i > start_idx + 5):\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "    if end_idx is None:\n",
        "        end_idx = len(lines)\n",
        "\n",
        "    # Process each data line\n",
        "    for i in range(start_idx, end_idx):\n",
        "        line = lines[i].strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Split by whitespace and process\n",
        "        parts = line.split()\n",
        "        if len(parts) < 6:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extract the index and coordinate type\n",
        "            index_part = parts[0]  # e.g., \"1.\"\n",
        "            coord_part = parts[1]  # e.g., \"B(C\"\n",
        "\n",
        "            # Get the index number\n",
        "            index = int(index_part.rstrip('.'))\n",
        "\n",
        "            # Extract coordinate type (B, A, or D)\n",
        "            coord_type = coord_part[0]\n",
        "            if coord_type not in ['B', 'A', 'D']:\n",
        "                continue\n",
        "\n",
        "            # Find the definition by looking for parentheses\n",
        "            definition_start = line.find('(')\n",
        "            definition_end = line.find(')')\n",
        "            if definition_start == -1 or definition_end == -1:\n",
        "                continue\n",
        "\n",
        "            definition = line[definition_start+1:definition_end]\n",
        "\n",
        "            # Extract the numerical values (last 4 columns)\n",
        "            # Find all floating point numbers in the line\n",
        "            numbers = re.findall(r'[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?', line)\n",
        "            if len(numbers) < 4:\n",
        "                continue\n",
        "\n",
        "            # Take the last 4 numbers as OldVal, dE/dq, Step, FinalVal\n",
        "            old_val = parse_float(numbers[-4])\n",
        "            dedq = parse_float(numbers[-3])\n",
        "            step = parse_float(numbers[-2])\n",
        "            final_val = parse_float(numbers[-1])\n",
        "\n",
        "            if None in (old_val, dedq, step, final_val):\n",
        "                continue\n",
        "\n",
        "            # Clean up the atom definition\n",
        "            atoms = definition.replace(' ', '').replace(',', '-')\n",
        "\n",
        "            # Categorize by coordinate type\n",
        "            if coord_type == 'B':\n",
        "                key = \"bonds\"\n",
        "                unit = \"Angstrom\"\n",
        "            elif coord_type == 'A':\n",
        "                key = \"angles\"\n",
        "                unit = \"Degrees\"\n",
        "            else:  # coord_type == 'D'\n",
        "                key = \"dihedrals\"\n",
        "                unit = \"Degrees\"\n",
        "\n",
        "            internal_data[key].append({\n",
        "                \"Index\": index,\n",
        "                \"Type\": coord_type,\n",
        "                \"Definition\": f\"{coord_type}({definition})\",\n",
        "                \"Atoms\": atoms,\n",
        "                \"OldVal\": old_val,\n",
        "                \"dE_dq\": dedq,\n",
        "                \"Step\": step,\n",
        "                \"FinalVal\": final_val,\n",
        "                \"Unit\": unit\n",
        "            })\n",
        "\n",
        "        except (ValueError, IndexError) as e:\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrames if requested\n",
        "    if as_df:\n",
        "        return {\n",
        "            \"bonds\": pd.DataFrame(internal_data[\"bonds\"]),\n",
        "            \"angles\": pd.DataFrame(internal_data[\"angles\"]),\n",
        "            \"dihedrals\": pd.DataFrame(internal_data[\"dihedrals\"])\n",
        "        }\n",
        "\n",
        "    return internal_data\n",
        "\n",
        "# ---------- Master Parser ----------\n",
        "def parse_orca_output(filename, as_df=False):\n",
        "    try:\n",
        "        with open(filename, \"r\") as f:\n",
        "            text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        # Return minimal result with just filename if file not found\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"geometry\": None,\n",
        "            \"cart_coords\": pd.DataFrame(columns=[\"atom\", \"x\", \"y\", \"z\", \"smiles\"]) if as_df else None,\n",
        "            \"smiles\": None,\n",
        "            \"int_coords\": pd.DataFrame(columns=[\"atom\", \"bond\", \"angle\", \"dihedral\"]) if as_df else None,\n",
        "            \"orbitals\": pd.DataFrame() if as_df else None,\n",
        "            \"vibrations\": pd.DataFrame(columns=[\"freq_cm-1\", \"img\"]) if as_df else None,\n",
        "            \"ir_spectrum\": pd.DataFrame(columns=[\"freq_cm-1\", \"eps\", \"intensity_km/mol\", \"t2\"]) if as_df else None,\n",
        "            \"raman_spectrum\": pd.DataFrame() if as_df else None,\n",
        "            \"gibbs_energy_Eh\": None,\n",
        "            \"single_point_energy_Eh\": None,\n",
        "            \"tddft_states\": pd.DataFrame() if as_df else None,\n",
        "            \"electric_dipole_spectrum\": {\"abs\": pd.DataFrame(), \"soc\": pd.DataFrame()} if as_df else {\"abs\": [], \"soc\": []},\n",
        "            \"velocity_dipole_spectrum\": {\"abs\": pd.DataFrame(), \"soc\": pd.DataFrame()} if as_df else {\"abs\": [], \"soc\": []},\n",
        "            \"nmr_data\": {\"shielding\": pd.DataFrame(), \"coupling\": pd.DataFrame()} if as_df else {\"shielding\": [], \"coupling\": []},\n",
        "            \"mulliken_charges\": pd.DataFrame() if as_df else None,\n",
        "            \"internal\": {\"bonds\": pd.DataFrame(), \"angles\": pd.DataFrame(), \"dihedrals\": pd.DataFrame()} if as_df else {\"bonds\": [], \"angles\": [], \"dihedrals\": []},\n",
        "            \"spectrum_file\": {}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error reading ORCA output file '{filename}': {str(e)}\")\n",
        "\n",
        "    # Absorption / Emission type\n",
        "    esd_flag = parse_esd_flag(text)\n",
        "\n",
        "    # Fallback from filename if missing\n",
        "    if esd_flag is None:\n",
        "        if 'ahas' in filename.lower():\n",
        "            esd_flag = 'AHAS'\n",
        "        elif 'ah' in filename.lower():\n",
        "            esd_flag = 'AH'\n",
        "        elif 'vg' in filename.lower():\n",
        "            esd_flag = 'VG'\n",
        "        elif 'phosp' in filename.lower():\n",
        "            esd_flag = 'PHOSP'\n",
        "        elif 'fluor' in filename.lower():\n",
        "            esd_flag = 'FLUOR'\n",
        "\n",
        "    # Parse geometry\n",
        "    geometry_info = parse_geometry_info(text)\n",
        "    cart_coords = parse_last_cartesian(text)\n",
        "    smiles = coords_to_smiles(cart_coords) if cart_coords else None\n",
        "\n",
        "    # Cartesian output\n",
        "    if as_df and cart_coords:\n",
        "        cart_df = pd.DataFrame(cart_coords, columns=[\"atom\", \"x\", \"y\", \"z\"])\n",
        "        if smiles:\n",
        "            cart_df[\"smiles\"] = smiles\n",
        "        cart_coords_output = cart_df\n",
        "    elif cart_coords:\n",
        "        cart_coords_output = cart_coords\n",
        "    else:\n",
        "        cart_coords_output = pd.DataFrame(columns=[\"atom\", \"x\", \"y\", \"z\", \"smiles\"]) if as_df else None\n",
        "\n",
        "    # Enhanced spectrum file search\n",
        "    spectrum_file = {}\n",
        "    base_filename = os.path.splitext(filename)[0]  # Remove .out extension\n",
        "\n",
        "    # Create comprehensive list of potential spectrum file patterns\n",
        "    spectrum_file_patterns = []\n",
        "\n",
        "    # 1. Direct filename match (e.g., cbzs0p.spectrum for cbzs0p.out)\n",
        "    spectrum_file_patterns.append(f\"{base_filename}.spectrum\")\n",
        "\n",
        "    # 2. Filename with detected ESD flag (e.g., cbzs0p_vg.spectrum)\n",
        "    if esd_flag:\n",
        "        spectrum_file_patterns.append(f\"{base_filename}_{esd_flag.lower()}.spectrum\")\n",
        "\n",
        "    # 3. If geometry info is available, use that base name\n",
        "    if geometry_info and geometry_info.get(\"filename\"):\n",
        "        geom_base = geometry_info[\"filename\"]\n",
        "        spectrum_file_patterns.append(f\"{geom_base}.spectrum\")\n",
        "        if esd_flag:\n",
        "            spectrum_file_patterns.append(f\"{geom_base}_{esd_flag.lower()}.spectrum\")\n",
        "\n",
        "    # 4. Special handling for files that might contain known ESD flags in their name\n",
        "    filename_base = os.path.basename(filename).lower()\n",
        "    if any(flag in filename_base for flag in ['vg', 'ah', 'ahas', 'phosp', 'fluor']):\n",
        "        calc_type = esd_flag\n",
        "        if 'vg' in filename_base:\n",
        "            calc_type = 'VG'\n",
        "        elif 'ahas' in filename_base:\n",
        "            calc_type = 'AHAS'\n",
        "        elif 'ah' in filename_base:\n",
        "            calc_type = 'AH'\n",
        "        elif 'phosp' in filename_base:\n",
        "            calc_type = 'PHOSP'\n",
        "        elif 'fluor' in filename_base:\n",
        "            calc_type = 'FLUOR'\n",
        "\n",
        "        if calc_type:\n",
        "            spectrum_file_patterns.append(f\"{base_filename}.spectrum\")\n",
        "            esd_flag = calc_type  # Update esd_flag for proper parsing\n",
        "\n",
        "    # Try each pattern until we find a valid spectrum file\n",
        "    for pattern in spectrum_file_patterns:\n",
        "        if os.path.exists(pattern):\n",
        "            spectrum_data = parse_spectrum_file(pattern, spectrum_type=esd_flag, as_df=as_df)\n",
        "            if (as_df and not spectrum_data.empty) or (not as_df and spectrum_data):\n",
        "                spectrum_file[esd_flag if esd_flag else \"UNKNOWN\"] = spectrum_data\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"filename\": filename,\n",
        "        \"geometry\": geometry_info,\n",
        "        \"cart_coords\": cart_coords_output,\n",
        "        \"smiles\": smiles,\n",
        "        \"int_coords\": parse_last_internal(text) if not as_df else pd.DataFrame(\n",
        "            parse_last_internal(text) or [], columns=[\"atom\", \"bond\", \"angle\", \"dihedral\"]),\n",
        "        \"orbitals\": parse_last_orbitals(text, as_df=as_df),\n",
        "        \"vibrations\": parse_last_vibrations(text) if not as_df else pd.DataFrame(\n",
        "            parse_last_vibrations(text) or [], columns=[\"freq_cm-1\", \"img\"]),\n",
        "        \"ir_spectrum\": parse_ir_spectrum(text) if not as_df else pd.DataFrame(\n",
        "            parse_ir_spectrum(text) or [], columns=[\"freq_cm-1\", \"eps\", \"intensity_km/mol\", \"t2\"]),\n",
        "        \"raman_spectrum\": parse_raman_spectrum(text, as_df=as_df),\n",
        "        \"gibbs_energy_Eh\": parse_gibbs_energy(text),\n",
        "        \"single_point_energy_Eh\": parse_single_point_energy(text),\n",
        "        \"tddft_states\": parse_tddft_states(text, parse_last_orbitals(text)),\n",
        "        \"electric_dipole_spectrum\": parse_electric_dipole_spectrum(text, as_df=as_df),\n",
        "        \"velocity_dipole_spectrum\": parse_velocity_dipole_spectrum(text, as_df=as_df),\n",
        "        \"nmr_data\": parse_nmr_data(text, as_df=as_df),\n",
        "        \"mulliken_charges\": parse_mulliken_charges(text, as_df=as_df),\n",
        "        \"internal\": parse_internal(text, as_df=as_df),\n",
        "        \"spectrum_file\": spectrum_file\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bM5XFUgWjq3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Download Dataset"
      ],
      "metadata": {
        "id": "-Q7UpkEeyNqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title\n",
        "# CONFIGURATION\n",
        "REPO_ID = \"JauharMz/Orca\"  # <-- CHANGE THIS to your target dataset\n",
        "DESTINATION_FOLDER = \"./chem\"\n",
        "\n",
        "# 3. Download the dataset preserving folder structure\n",
        "snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=DESTINATION_FOLDER,\n",
        "    local_dir_use_symlinks=False, # Important for Colab/Drive to see actual files\n",
        "    # Optional: Only download specific folders (remove comment to use)\n",
        "    # allow_patterns=[\"data/train/*\", \"*.csv\"]\n",
        ")\n",
        "\n",
        "print(f\"Download complete! Check the folder: {DESTINATION_FOLDER}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ACvd-f__lfeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Dataset Preprocess"
      ],
      "metadata": {
        "id": "di2XAnCNyVCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same folder name you used in the previous step\n",
        "dataset_root = Path(DESTINATION_FOLDER)\n",
        "\n",
        "# 2. Automatically find ALL .out files in every subfolder (recursive)\n",
        "# rglob = recursive glob. \"*.out\" matches any file ending in .out\n",
        "all_files = list(dataset_root.rglob(\"*.out\"))\n",
        "\n",
        "print(f\"Found {len(all_files)} files.\")\n",
        "\n",
        "molecules = []\n",
        "\n",
        "# 3. Loop through the found files\n",
        "for file_path in tqdm(all_files, desc=\"Parsing ORCA files\"):\n",
        "\n",
        "    # Convert Path object to string for your parser\n",
        "    file_path_str = str(file_path)\n",
        "\n",
        "    # Optional: Filter specifically if you only want files with 's0' or 's0p' in the name\n",
        "    # like in your example list (p1xs0p, CO2s0, etc.)\n",
        "    # if \"s0\" not in file_path.name:\n",
        "    #     continue\n",
        "\n",
        "    try:\n",
        "        # Run your parsing function\n",
        "        # Note: We pass the FULL path, not just the filename\n",
        "        mol_data = parse_orca_output(file_path_str)\n",
        "\n",
        "        # You might want to store the filename too for reference\n",
        "        # molecules.append({\"name\": file_path.name, \"data\": mol_data})\n",
        "\n",
        "        molecules.append(mol_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path.name}: {e}\")\n",
        "\n",
        "print(f\"Successfully parsed {len(molecules)} molecules.\")"
      ],
      "metadata": {
        "id": "q2SXBTCJmxfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(molecules)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iLuOXNMDnBx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Data Visualization"
      ],
      "metadata": {
        "id": "V50QFjDmyZpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.01 Molecul Visualization"
      ],
      "metadata": {
        "id": "T0fs3NmtzNn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import py3Dmol\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import base64\n",
        "\n",
        "datasets = [[0, 1, 2, 3, 4, 5],\n",
        "            [6, 7, 8, 9, 10, 11],\n",
        "            [12, 13, 14, 15, 16, 17],\n",
        "            [18, 19, 20, 21, 22, 23],\n",
        "            [24, 25, 26, 27, 28, 29]]\n",
        "\n",
        "labels_input = [[\"p1x\", \"p2x\", \"p3x\", \"p4x\", \"p5x\", \"p6x\"],\n",
        "                [\"p1a\", \"p2a\", \"p3a\", \"p4a\", \"p5a\", \"p6a\"],\n",
        "                [\"p1b\", \"p2b\", \"p3b\", \"p4b\", \"p5b\", \"p6b\"],\n",
        "                [\"p1c\", \"p2c\", \"p3c\", \"p4c\", \"p5c\", \"p6c\"],\n",
        "                [\"p1d\", \"p2d\", \"p3d\", \"p4d\", \"p5d\", \"p6d\"]]\n",
        "\n",
        "molsPerRow = max(len(row) for row in datasets)\n",
        "molecules = []\n",
        "labels_list = []\n",
        "\n",
        "for i in range(len(datasets)):\n",
        "    for j in range(len(datasets[i])):\n",
        "        idx = datasets[i][j]\n",
        "        if idx in df.index:\n",
        "            mol = Chem.MolFromSmiles(df['smiles'][idx])\n",
        "            if mol:\n",
        "                AllChem.Compute2DCoords(mol)\n",
        "                molecules.append(mol)\n",
        "            else:\n",
        "                molecules.append(None)\n",
        "            labels_list.append(labels_input[i][j])\n",
        "        else:\n",
        "            molecules.append(None)\n",
        "            labels_list.append(\"\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. PRE-CALCULATION (Runs once)\n",
        "# -----------------------------------------------------------\n",
        "print(\" Processing Data...\")\n",
        "\n",
        "prepared_data = []\n",
        "flat_data_map = {}\n",
        "rows = len(datasets)\n",
        "cols = len(datasets[0])\n",
        "mol_idx = 0\n",
        "\n",
        "def get_b64_image(mol):\n",
        "    try:\n",
        "        d2d = rdMolDraw2D.MolDraw2DSVG(280, 280)\n",
        "        d2d.drawOptions().addAtomIndices = False\n",
        "        d2d.DrawMolecule(mol)\n",
        "        d2d.FinishDrawing()\n",
        "        svg = d2d.GetDrawingText().replace('<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>', '')\n",
        "        return base64.b64encode(svg.encode('utf-8')).decode('utf-8')\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "for i in range(rows):\n",
        "    row_data = []\n",
        "    for j in range(cols):\n",
        "        cell_data = None\n",
        "        if mol_idx < len(molecules):\n",
        "            mol_2d = molecules[mol_idx]\n",
        "            label_text = labels_list[mol_idx]\n",
        "\n",
        "            if mol_2d:\n",
        "                try:\n",
        "                    # Properties & 3D\n",
        "                    mw = Descriptors.MolWt(mol_2d)\n",
        "                    logp = Descriptors.MolLogP(mol_2d)\n",
        "                    tpsa = Descriptors.TPSA(mol_2d)\n",
        "                    hbd = Descriptors.NumHDonors(mol_2d)\n",
        "                    hba = Descriptors.NumHAcceptors(mol_2d)\n",
        "\n",
        "                    m3d = Chem.Mol(mol_2d)\n",
        "                    m3d = Chem.AddHs(m3d)\n",
        "                    try: AllChem.ComputeGasteigerCharges(m3d)\n",
        "                    except: pass\n",
        "\n",
        "                    params = AllChem.ETKDGv3()\n",
        "                    params.useRandomCoords = True\n",
        "                    if AllChem.EmbedMolecule(m3d, params) == -1:\n",
        "                        AllChem.EmbedMolecule(m3d, randomSeed=42)\n",
        "                    AllChem.MMFFOptimizeMolecule(m3d)\n",
        "\n",
        "                    cell_data = {\n",
        "                        'block': Chem.MolToMolBlock(m3d),\n",
        "                        'mol': m3d, # Keep the object to read atoms later\n",
        "                        'label_3d': f\"{label_text}\\nLogP:{logp:.1f}\",\n",
        "                        'name': label_text,\n",
        "                        'stats': {\n",
        "                            'MW': mw, 'LogP': logp, 'TPSA': tpsa, 'HBD': hbd, 'HBA': hba,\n",
        "                            'SMILES': Chem.MolToSmiles(mol_2d),\n",
        "                            'img': get_b64_image(mol_2d)\n",
        "                        }\n",
        "                    }\n",
        "                    flat_data_map[label_text] = cell_data\n",
        "                except:\n",
        "                    cell_data = None\n",
        "\n",
        "        row_data.append(cell_data)\n",
        "        mol_idx += 1\n",
        "    prepared_data.append(row_data)\n",
        "\n",
        "print(\" Ready.\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. RENDERER\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def render_dashboard(style, color_mode, material, show_surf, opacity, show_info_lbl, show_atom_lbl, bg_color, spin):\n",
        "    view = py3Dmol.view(width=1000, height=1200, viewergrid=(rows, cols))\n",
        "    view.setBackgroundColor(bg_color)\n",
        "\n",
        "    # Determine text color based on background\n",
        "    text_col = 'black' if bg_color == '#ffffff' else 'white'\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            data = prepared_data[r][c]\n",
        "            if data:\n",
        "                view.addModel(data['block'], \"mol\", viewer=(r, c))\n",
        "\n",
        "                # --- COLORS ---\n",
        "                if color_mode == 'Electrostatic (Charge)':\n",
        "                    color_def = {'prop': 'partialCharge', 'gradient': 'rwb', 'min': -0.25, 'max': 0.25}\n",
        "                elif color_mode == 'Professional (Light Carbon)':\n",
        "                    color_def = {'colors': {'C': '#d3d3d3', 'H': 'white', 'N': '#3050F8', 'O': '#FF0D0D', 'S': '#FFFF30', 'F': '#90E050', 'Cl': '#1FF01F'}}\n",
        "                elif color_mode == 'Vibrant (Teal Carbon)':\n",
        "                    color_def = {'colors': {'C': '#008080', 'H': 'white', 'N': '#483D8B', 'O': '#FF4500'}}\n",
        "                else:\n",
        "                    color_def = 'Jmol'\n",
        "\n",
        "                # --- STYLE ---\n",
        "                mat_spec = {'specular': 1, 'shininess': 50} if material == 'Shiny (Plastic)' else {}\n",
        "\n",
        "                # Helper to apply style map safely\n",
        "                def apply_style(s_type, s_data):\n",
        "                    spec = {s_type: {**s_data, **mat_spec}}\n",
        "                    if style == 'Ball & Stick':\n",
        "                        spec['sphere'] = {'scale': 0.25, **s_data, **mat_spec}\n",
        "                    view.setStyle(spec, viewer=(r, c))\n",
        "\n",
        "                if isinstance(color_def, dict) and 'colors' in color_def:\n",
        "                    if style == 'Ball & Stick':\n",
        "                        view.setStyle({'stick':{'radius':0.15, 'colorscheme':color_def, **mat_spec}, 'sphere':{'scale':0.25, 'colorscheme':color_def, **mat_spec}}, viewer=(r, c))\n",
        "                    elif style == 'Spacefill': view.setStyle({'sphere':{'scale':0.85, 'colorscheme':color_def, **mat_spec}}, viewer=(r, c))\n",
        "                    elif style == 'Stick': view.setStyle({'stick':{'radius':0.15, 'colorscheme':color_def, **mat_spec}}, viewer=(r, c))\n",
        "                else:\n",
        "                    # Standard/Gradient\n",
        "                    base = {'color': color_def} if isinstance(color_def, str) else {'color': color_def} # simplified logic\n",
        "                    if style == 'Ball & Stick':\n",
        "                         view.setStyle({'stick':{'radius':0.15, 'color':color_def, **mat_spec}, 'sphere':{'scale':0.25, 'color':color_def, **mat_spec}}, viewer=(r, c))\n",
        "                    elif style == 'Spacefill': view.setStyle({'sphere':{'scale':0.85, 'color':color_def, **mat_spec}}, viewer=(r, c))\n",
        "                    elif style == 'Stick': view.setStyle({'stick':{'radius':0.15, 'color':color_def, **mat_spec}}, viewer=(r, c))\n",
        "\n",
        "                # --- SURFACE ---\n",
        "                if show_surf:\n",
        "                    surf_col = 'white'\n",
        "                    if color_mode == 'Electrostatic (Charge)': surf_col = color_def\n",
        "                    view.addSurface(py3Dmol.VDW, {'opacity': opacity, 'color': surf_col}, viewer=(r, c))\n",
        "\n",
        "                # --- INFO LABELS (Box Label) ---\n",
        "                if show_info_lbl:\n",
        "                    view.addLabel(data['label_3d'],\n",
        "                                  {'fontSize': 10, 'fontColor': 'white', 'backgroundColor': 'black',\n",
        "                                   'backgroundOpacity': 0.6, 'position': {'x': -3, 'y': 3, 'z': 0}},\n",
        "                                  viewer=(r, c))\n",
        "\n",
        "                # --- ATOM LABELS (The Missing Feature!) ---\n",
        "                if show_atom_lbl:\n",
        "                    mol = data['mol']\n",
        "                    conf = mol.GetConformer()\n",
        "                    for atom in mol.GetAtoms():\n",
        "                        pos = conf.GetAtomPosition(atom.GetIdx())\n",
        "                        sym = atom.GetSymbol()\n",
        "                        # Only label heavy atoms (ignore Hydrogen to keep it clean) usually better\n",
        "                        if sym != 'H':\n",
        "                            view.addLabel(sym,\n",
        "                                          {'fontSize': 10, 'fontColor': text_col, 'showBackground': False,\n",
        "                                           'position': {'x': pos.x, 'y': pos.y, 'z': pos.z}},\n",
        "                                          viewer=(r, c))\n",
        "\n",
        "                view.zoomTo(viewer=(r, c))\n",
        "\n",
        "    if spin: view.spin(True, speed=0.2)\n",
        "    view.show()\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. WIDGETS\n",
        "# -----------------------------------------------------------\n",
        "inspector_out = widgets.Output()\n",
        "download_out = widgets.Output()\n",
        "\n",
        "def on_inspect(change):\n",
        "    name = change['new']\n",
        "    d = flat_data_map.get(name)\n",
        "    with download_out:\n",
        "        clear_output()\n",
        "        if d: display(HTML(f\" <a href='#' onclick='return false;'>{name}.mol (Use button above)</a>\"))\n",
        "\n",
        "    with inspector_out:\n",
        "        clear_output()\n",
        "        if d:\n",
        "            s = d['stats']\n",
        "            html = f\"\"\"\n",
        "            <div style=\"font-family:sans-serif; border:2px solid #ddd; padding:15px; background:#fff; display:flex; gap:20px;\">\n",
        "                <div style=\"text-align:center;\"><img src=\"data:image/svg+xml;base64,{s['img']}\" style=\"border:1px solid #eee;\"></div>\n",
        "                <div style=\"flex-grow:1;\">\n",
        "                    <h3 style=\"margin:0;\">{name}</h3>\n",
        "                    <table style=\"width:100%; margin-top:10px;\">\n",
        "                        <tr><td>MW:</td><td><b>{s['MW']:.2f}</b></td><td>LogP:</td><td><b>{s['LogP']:.2f}</b></td></tr>\n",
        "                        <tr><td>TPSA:</td><td><b>{s['TPSA']:.1f}</b></td><td>H-Bonds:</td><td><b>{s['HBD']}/{s['HBA']}</b></td></tr>\n",
        "                    </table>\n",
        "                    <br>SMILES: <input type=\"text\" value=\"{s['SMILES']}\" style=\"width:100%;\" readonly>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(html))\n",
        "\n",
        "# Controls\n",
        "style_w = widgets.Dropdown(options=['Ball & Stick', 'Stick', 'Spacefill'], value='Ball & Stick', description='Style')\n",
        "color_w = widgets.Dropdown(options=['Electrostatic (Charge)', 'Professional (Light Carbon)', 'Standard (Jmol)'], value='Electrostatic (Charge)', description='Color')\n",
        "mat_w = widgets.Dropdown(options=['Shiny (Plastic)', 'Matte'], value='Shiny (Plastic)', description='Material')\n",
        "select_w = widgets.Dropdown(options=flat_data_map.keys(), description=' INSPECT')\n",
        "\n",
        "# Toggles\n",
        "surf_w = widgets.Checkbox(value=False, description='Surface')\n",
        "lbl_w = widgets.Checkbox(value=True, description='Box Labels')\n",
        "atom_lbl_w = widgets.Checkbox(value=False, description='Atom Text') # <--- NEW WIDGET\n",
        "spin_w = widgets.Checkbox(value=False, description='Spin')\n",
        "opac_w = widgets.FloatSlider(value=0.5, min=0.1, max=1.0, description='Opac')\n",
        "bg_w = widgets.ColorPicker(value='#ffffff', description='BG Color')\n",
        "\n",
        "select_w.observe(on_inspect, names='value')\n",
        "\n",
        "controls = widgets.VBox([\n",
        "    widgets.HBox([style_w, color_w, mat_w]),\n",
        "    widgets.HBox([surf_w, opac_w, lbl_w, atom_lbl_w, spin_w, bg_w]), # Added atom_lbl_w here\n",
        "])\n",
        "\n",
        "viz_out = widgets.interactive_output(render_dashboard, {\n",
        "    'style': style_w, 'color_mode': color_w, 'material': mat_w,\n",
        "    'show_surf': surf_w, 'opacity': opac_w, 'show_info_lbl': lbl_w,\n",
        "    'show_atom_lbl': atom_lbl_w, 'bg_color': bg_w, 'spin': spin_w\n",
        "})\n",
        "\n",
        "display(controls, viz_out, HTML(\"<hr>\"), select_w, inspector_out)\n",
        "if flat_data_map: on_inspect({'new': list(flat_data_map.keys())[0]})"
      ],
      "metadata": {
        "id": "hjXUH5qxDYje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.02 Gibbs"
      ],
      "metadata": {
        "id": "7PAeRt4gzp1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. FIX DATA: Ensure df has 30 rows\n",
        "# ==========================================\n",
        "\n",
        "# Check if df exists, if not create it\n",
        "if 'df' not in locals():\n",
        "    # If starting from scratch, make 30 rows\n",
        "    df = pd.DataFrame({'gibbs_energy_Eh': np.linspace(-1000, -1000.1, 30)})\n",
        "else:\n",
        "    # If df exists but is too short (e.g., only 6 rows), expand it!\n",
        "    current_len = len(df)\n",
        "    required_len = 30\n",
        "    if current_len < required_len:\n",
        "        print(f\" Extending dataframe from {current_len} to {required_len} rows for visualization...\")\n",
        "        # Create dummy rows for the missing data\n",
        "        missing_count = required_len - current_len\n",
        "        dummy_data = np.linspace(-1000, -1000.1, missing_count)\n",
        "\n",
        "        # Append to existing df (assuming simple integer index)\n",
        "        new_rows = pd.DataFrame({'gibbs_energy_Eh': dummy_data},\n",
        "                                index=range(current_len, required_len))\n",
        "        df = pd.concat([df, new_rows])\n",
        "\n",
        "# ==========================================\n",
        "# 2. YOUR EXACT INPUTS\n",
        "# ==========================================\n",
        "datasets = [0, 6, 12, 18, 24,\n",
        "            1, 7, 13, 19, 25,\n",
        "            2, 8, 14, 20, 26,\n",
        "            3, 9, 15, 21, 27,\n",
        "            4, 10, 16, 22, 28,\n",
        "            5, 11, 17, 23, 29]\n",
        "\n",
        "labels_input = [\"p1x\", \"p1a\", \"p1b\", \"p1c\", \"p1d\",\n",
        "                \"p2x\", \"p2a\", \"p2b\", \"p2c\", \"p2d\",\n",
        "                \"p3x\", \"p3a\", \"p3b\", \"p3c\", \"p3d\",\n",
        "                \"p4x\", \"p4a\", \"p4b\", \"p4c\", \"p4d\",\n",
        "                \"p5x\", \"p5a\", \"p5b\", \"p5c\", \"p5d\",\n",
        "                \"p6x\", \"p6a\", \"p6b\", \"p6c\", \"p6d\"]\n",
        "\n",
        "hartree_to_kcalmol = 627.509\n",
        "\n",
        "# ==========================================\n",
        "# 3. PROCESS DATA\n",
        "# ==========================================\n",
        "gibbs_data = []\n",
        "for i, label in zip(datasets, labels_input):\n",
        "    try:\n",
        "        # Now this will work because df has 30 rows!\n",
        "        if i < len(df):\n",
        "            g = df[\"gibbs_energy_Eh\"].iloc[i] * hartree_to_kcalmol\n",
        "            gibbs_data.append((label, g))\n",
        "    except Exception as e:\n",
        "        print(f\" Skipped dataset {i} ({label}): {e}\")\n",
        "\n",
        "gibbs_df = pd.DataFrame(gibbs_data, columns=[\"Label\", \"Gibbs (kcal/mol)\"])\n",
        "\n",
        "# Reference = first dataset (p1x)\n",
        "ref_energy = gibbs_df[\"Gibbs (kcal/mol)\"].iloc[0]\n",
        "gibbs_df[\"DeltaG (kcal/mol)\"] = gibbs_df[\"Gibbs (kcal/mol)\"] - ref_energy\n",
        "gibbs_df[\"Step G\"] = gibbs_df[\"Gibbs (kcal/mol)\"].diff().fillna(0)\n",
        "\n",
        "# ==========================================\n",
        "# 4. PLOTLY VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_datasets = len(gibbs_df)\n",
        "x_positions = list(range(1, n_datasets + 1))\n",
        "overaxis = 0.35\n",
        "\n",
        "# --- A. Draw Connectors ---\n",
        "for i in range(n_datasets - 1):\n",
        "    x1 = x_positions[i] + overaxis\n",
        "    x2 = x_positions[i+1] - overaxis\n",
        "    y1 = gibbs_df[\"DeltaG (kcal/mol)\"].iloc[i]\n",
        "    y2 = gibbs_df[\"DeltaG (kcal/mol)\"].iloc[i+1]\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[x1, x2], y=[y1, y2],\n",
        "        mode='lines',\n",
        "        line=dict(color='gray', width=1.2, dash='dash'),\n",
        "        hoverinfo='skip',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# --- B. Draw Bars & Labels ---\n",
        "for i, x in enumerate(x_positions):\n",
        "    energy = gibbs_df[\"DeltaG (kcal/mol)\"].iloc[i]\n",
        "    step_dg = gibbs_df[\"Step G\"].iloc[i]\n",
        "    label_name = gibbs_df[\"Label\"].iloc[i]\n",
        "\n",
        "    if i == 0: label_text = \"0.00\"\n",
        "    else: label_text = f\"{step_dg:+.2f}\"\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[x - overaxis, x + overaxis],\n",
        "        y=[energy, energy],\n",
        "        mode='lines',\n",
        "        line=dict(color='black', width=3),\n",
        "        name=label_name,\n",
        "        hovertemplate=f\"<b>{label_name}</b><br>Relative: {energy:.2f}<br>Step: {step_dg:+.2f}<extra></extra>\",\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "    fig.add_annotation(\n",
        "        x=x, y=energy + 0.5,\n",
        "        text=label_text,\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"black\"),\n",
        "        yanchor=\"bottom\"\n",
        "    )\n",
        "\n",
        "# --- C. Custom Axis Arrow ---\n",
        "y_min = gibbs_df[\"DeltaG (kcal/mol)\"].min() - 5\n",
        "y_max = gibbs_df[\"DeltaG (kcal/mol)\"].max() + 5\n",
        "arrow_x = 0.5\n",
        "\n",
        "fig.add_annotation(\n",
        "    x=arrow_x, y=y_max, ax=arrow_x, ay=y_min,\n",
        "    xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n",
        "    showarrow=True, arrowhead=2, arrowsize=1.5, arrowwidth=1.5, arrowcolor=\"black\"\n",
        ")\n",
        "fig.add_annotation(\n",
        "    x=arrow_x, y=(y_max + y_min)/2,\n",
        "    text=\"G (kcal/mol)\", textangle=-90,\n",
        "    showarrow=False, font=dict(size=12, color=\"black\"),\n",
        "    xanchor=\"right\", xshift=-10\n",
        ")\n",
        "\n",
        "# --- D. Final Layout ---\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='white',\n",
        "    width=max(800, n_datasets * 45),\n",
        "    height=600,\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=x_positions,\n",
        "        ticktext=labels_input,\n",
        "        showgrid=False, zeroline=False, showline=True,\n",
        "        linecolor='black',\n",
        "        tickfont=dict(size=11),\n",
        "        range=[0, n_datasets + 1]\n",
        "    ),\n",
        "    yaxis=dict(showgrid=False, zeroline=False, showline=False, showticklabels=False, range=[y_min, y_max + 2]),\n",
        "    margin=dict(l=50, r=20, t=40, b=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "J0Zl03r3Lez2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict\n",
        "\n",
        "# ================================================================\n",
        "# 1. SETUP & DUMMY DATA (For Safety)\n",
        "# ================================================================\n",
        "if 'df' not in locals():\n",
        "    # Generating dummy data for 30 points to ensure code runs\n",
        "    dummy_vals = np.linspace(-1000, -1000.5, 30)\n",
        "    df = pd.DataFrame({'gibbs_energy_Eh': dummy_vals})\n",
        "\n",
        "# ================================================================\n",
        "# 2. CONSTANTS & MAPPINGS (Your Exact Data)\n",
        "# ================================================================\n",
        "hartree_to_kcalmol = 627.5095\n",
        "\n",
        "species_energy_Eh = {\n",
        "    \"CO2\": -188.569441, \"H2O\": -76.433514, \"HCO3\": -264.317631,\n",
        "    \"OH\": -75.736911, \"OMe\": -115.038597,\n",
        "}\n",
        "species_energy = {sp: Eh * hartree_to_kcalmol for sp, Eh in species_energy_Eh.items()}\n",
        "\n",
        "states = {\n",
        "    \"p1x\": 0,  \"p2x\": 1,  \"p3x\": 2,  \"p4x\": 3,  \"p5x\": 4,  \"p6x\": 5,\n",
        "    \"p1a\": 6,  \"p2a\": 7,  \"p3a\": 8,  \"p4a\": 9,  \"p5a\": 10, \"p6a\": 11,\n",
        "    \"p1b\": 12, \"p2b\": 13, \"p3b\": 14, \"p4b\": 15, \"p5b\": 16, \"p6b\": 17,\n",
        "    \"p1c\": 18, \"p2c\": 19, \"p3c\": 20, \"p4c\": 21, \"p5c\": 22, \"p6c\": 23,\n",
        "    \"p1d\": 24, \"p2d\": 25, \"p3d\": 26, \"p4d\": 27, \"p5d\": 28, \"p6d\": 29,\n",
        "}\n",
        "\n",
        "grid = [\n",
        "    [\"p1x\", None, \"p2x\", \"p3x\", \"p4x\", \"p5x\", None],\n",
        "    [\"p1x\", None, None, None, None, None, \"p6x\"],\n",
        "    [\"p1x\", \"p1a\", \"p2a\", \"p3a\", \"p4a\", \"p5a\", None],\n",
        "    [\"p1x\", \"p1a\", None, None, None, None, \"p6a\"],\n",
        "]\n",
        "\n",
        "step_corrections = {\n",
        "    (\"p1x\", \"p2x\"): {\"add\": {\"OH\": 4}, \"remove\": {\"H2O\": 3}},\n",
        "    (\"p2x\", \"p3x\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p3x\", \"p4x\"): {\"remove\": {\"CO2\": 1}},\n",
        "    (\"p4x\", \"p5x\"): {\"add\": {\"OH\": 1}, \"remove\": {\"HCO3\": 1}},\n",
        "    (\"p1x\", \"p6x\"): {\"add\": {\"OH\": 1}, \"remove\": {\"H2O\": 1, \"OMe\": 1}},\n",
        "\n",
        "    (\"p1x\", \"p1a\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p1a\", \"p2a\"): {\"add\": {\"OH\": 4}, \"remove\": {\"H2O\": 3}},\n",
        "    (\"p2a\", \"p3a\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p3a\", \"p4a\"): {\"remove\": {\"CO2\": 1}},\n",
        "    (\"p4a\", \"p5a\"): {\"add\": {\"OH\": 1}, \"remove\": {\"HCO3\": 1}},\n",
        "    (\"p1a\", \"p6a\"): {\"add\": {\"OH\": 1}, \"remove\": {\"H2O\": 1, \"OMe\": 1}},\n",
        "\n",
        "    (\"p1x\", \"p1b\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p1b\", \"p2b\"): {\"add\": {\"OH\": 4}, \"remove\": {\"H2O\": 3}},\n",
        "    (\"p2b\", \"p3b\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p3b\", \"p4b\"): {\"remove\": {\"CO2\": 1}},\n",
        "    (\"p4b\", \"p5b\"): {\"add\": {\"OH\": 1}, \"remove\": {\"HCO3\": 1}},\n",
        "    (\"p1b\", \"p6b\"): {\"add\": {\"OH\": 1}, \"remove\": {\"H2O\": 1, \"OMe\": 1}},\n",
        "\n",
        "    (\"p1x\", \"p1c\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p1c\", \"p2c\"): {\"add\": {\"OH\": 4}, \"remove\": {\"H2O\": 3}},\n",
        "    (\"p2c\", \"p3c\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p3c\", \"p4c\"): {\"remove\": {\"CO2\": 1}},\n",
        "    (\"p4c\", \"p5c\"): {\"add\": {\"OH\": 1}, \"remove\": {\"HCO3\": 1}},\n",
        "    (\"p1c\", \"p6c\"): {\"add\": {\"OH\": 1}, \"remove\": {\"H2O\": 1, \"OMe\": 1}},\n",
        "\n",
        "    (\"p1x\", \"p1d\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p1d\", \"p2d\"): {\"add\": {\"OH\": 4}, \"remove\": {\"H2O\": 3}},\n",
        "    (\"p2d\", \"p3d\"): {\"add\": {\"OH\": 2}, \"remove\": {\"H2O\": 1}},\n",
        "    (\"p3d\", \"p4d\"): {\"remove\": {\"CO2\": 1}},\n",
        "    (\"p4d\", \"p5d\"): {\"add\": {\"OH\": 1}, \"remove\": {\"HCO3\": 1}},\n",
        "    (\"p1d\", \"p6d\"): {\"add\": {\"OH\": 1}, \"remove\": {\"H2O\": 1, \"OMe\": 1}},\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 3. ENERGY LOGIC (Preserved)\n",
        "# ================================================================\n",
        "def raw_G_kcal(label, df):\n",
        "    # Safe check in case df is smaller than index\n",
        "    idx = states[label]\n",
        "    if idx >= len(df): return 0.0\n",
        "    return float(df[\"gibbs_energy_Eh\"].iloc[idx]) * hartree_to_kcalmol\n",
        "\n",
        "def compute_step_deltaG(prev_label, curr_label, df):\n",
        "    if prev_label is None: return 0.0\n",
        "    Gp = raw_G_kcal(curr_label, df)\n",
        "    Gr = raw_G_kcal(prev_label, df)\n",
        "    mapping = step_corrections.get((prev_label, curr_label))\n",
        "    if mapping:\n",
        "        for sp, n in mapping.get(\"remove\", {}).items(): Gp += n * species_energy[sp]\n",
        "        for sp, n in mapping.get(\"add\", {}).items(): Gr += n * species_energy[sp]\n",
        "    return Gp - Gr\n",
        "\n",
        "def compute_pathway(labels, df, mode):\n",
        "    values = []\n",
        "    prev = None\n",
        "    cum = 0.0\n",
        "    for lab in labels:\n",
        "        if lab is None:\n",
        "            values.append(None)\n",
        "            continue\n",
        "        if prev is None:\n",
        "            values.append(0.0) # Always start at 0\n",
        "            prev = lab\n",
        "            continue\n",
        "        dG = compute_step_deltaG(prev, lab, df)\n",
        "        if mode == \"cumulative\":\n",
        "            cum += dG\n",
        "            values.append(cum)\n",
        "        else:\n",
        "            values.append(dG)\n",
        "        prev = lab\n",
        "    return values\n",
        "\n",
        "# ================================================================\n",
        "# 4. PLOTLY VISUALIZATION\n",
        "# ================================================================\n",
        "\n",
        "def plot_interactive(grid, df, mode=\"cumulative\"):\n",
        "\n",
        "    fig = go.Figure()\n",
        "    bar_w = 0.32\n",
        "\n",
        "    # --- 1. Calculate All Points ---\n",
        "    all_points = []\n",
        "\n",
        "    # We iterate grid to get lines and points\n",
        "    for labels in grid:\n",
        "        vals = compute_pathway(labels, df, mode)\n",
        "\n",
        "        # Filter valid points for lines\n",
        "        pts = [(i + 1, E, lab) for i, (lab, E) in enumerate(zip(labels, vals)) if lab and E is not None]\n",
        "        all_points.extend(pts)\n",
        "\n",
        "        # Draw Connectors (Dashed Lines)\n",
        "        # We do this path-by-path to maintain the correct connectivity\n",
        "        for k in range(len(pts) - 1):\n",
        "            x1, y1, _ = pts[k]\n",
        "            x2, y2, _ = pts[k+1]\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[x1 + bar_w, x2 - bar_w],\n",
        "                y=[y1, y2],\n",
        "                mode='lines',\n",
        "                line=dict(color='gray', width=1, dash='dash'),\n",
        "                hoverinfo='skip',\n",
        "                showlegend=False\n",
        "            ))\n",
        "\n",
        "    # --- 2. Group Unique Bars (The \"Uniq\" Logic) ---\n",
        "    # We don't want to draw 5 black bars on top of each other for 'p1x'\n",
        "    unique_bars = {}\n",
        "\n",
        "    for x, E, lab in all_points:\n",
        "        # Create a unique key based on X position and Energy (rounded)\n",
        "        key = (x, round(E, 6))\n",
        "        unique_bars[key] = (E, lab) # Store the exact energy and label\n",
        "\n",
        "    # --- 3. Draw Unique Bars & Labels ---\n",
        "    for (x, _), (E, lab) in unique_bars.items():\n",
        "\n",
        "        # A. Horizontal Bar\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[x - bar_w, x + bar_w],\n",
        "            y=[E, E],\n",
        "            mode='lines',\n",
        "            line=dict(color='black', width=3),\n",
        "            # Interactive Tooltip\n",
        "            hovertemplate=f\"<b>{lab.upper()}</b><br>G: {E:+.2f} kcal/mol<extra></extra>\",\n",
        "            showlegend=False\n",
        "        ))\n",
        "\n",
        "        # B. Energy Label (Above Bar)\n",
        "        fig.add_annotation(\n",
        "            x=x, y=E,\n",
        "            text=f\"{E:+.1f}\",\n",
        "            showarrow=False,\n",
        "            font=dict(size=10, color=\"black\"),\n",
        "            yshift=10\n",
        "        )\n",
        "\n",
        "        # C. Name Label (Above Energy)\n",
        "        fig.add_annotation(\n",
        "            x=x, y=E,\n",
        "            text=f\"<b>{lab.upper()}</b>\",\n",
        "            showarrow=False,\n",
        "            font=dict(size=12, color=\"black\"),\n",
        "            yshift=25 # Higher up\n",
        "        )\n",
        "\n",
        "    # --- 4. Custom Arrow (Bottom Left) ---\n",
        "    # Get bounds to position arrow dynamically\n",
        "    energies = [E for E, _ in unique_bars.values()]\n",
        "    x_max = max([x for (x, _), _ in unique_bars.items()])\n",
        "\n",
        "    y_min, y_max = min(energies), max(energies)\n",
        "    y_range = y_max - y_min\n",
        "\n",
        "    arrow_x = 0.8 # Slightly offset from x=1\n",
        "    arrow_y_start = y_min + (0.10 * y_range)\n",
        "    arrow_y_end = y_min + (0.25 * y_range)\n",
        "\n",
        "    # Arrow Head\n",
        "    fig.add_annotation(\n",
        "        x=arrow_x, y=arrow_y_end,\n",
        "        ax=arrow_x, ay=arrow_y_start,\n",
        "        xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n",
        "        showarrow=True, arrowhead=2, arrowsize=1.5, arrowwidth=1.5, arrowcolor=\"black\"\n",
        "    )\n",
        "\n",
        "    # Text \"Delta G_sol\"\n",
        "    fig.add_annotation(\n",
        "        x=arrow_x, y=(arrow_y_start + arrow_y_end) / 2,\n",
        "        text=\"G<sub>sol</sub><br>(kcal/mol)\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=12, color=\"black\"),\n",
        "        xanchor=\"left\", xshift=10\n",
        "    )\n",
        "\n",
        "    # --- 5. Layout ---\n",
        "    fig.update_layout(\n",
        "        plot_bgcolor='white',\n",
        "        width=1000,\n",
        "        height=600,\n",
        "        xaxis=dict(\n",
        "            showgrid=False, zeroline=False, showline=False,\n",
        "            showticklabels=False, # Hiding numbers 1,2,3,4...\n",
        "            range=[0.5, x_max + 1.5]\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            showgrid=False, zeroline=False, showline=False,\n",
        "            showticklabels=False, # Hiding energy numbers\n",
        "            range=[y_min - (0.1*y_range), y_max + (0.15*y_range)]\n",
        "        ),\n",
        "        margin=dict(l=20, r=20, t=20, b=20)\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# ================================================================\n",
        "# 5. RUN\n",
        "# ================================================================\n",
        "plot_interactive(grid, df, mode=\"cumulative\")"
      ],
      "metadata": {
        "id": "y_EKzZISOjY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.03 Energy Gap"
      ],
      "metadata": {
        "id": "4WxpV8cY0I8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ==========================================\n",
        "# 1. MOCK DATA GENERATION (DELETE THIS BLOCK WITH YOUR REAL DATA)\n",
        "# ==========================================\n",
        "# Creating a fake 'df' structure to match your indices (0 to 29)\n",
        "num_total_datasets = 30\n",
        "mock_data = []\n",
        "for k in range(num_total_datasets):\n",
        "    # Random energies: shift them slightly based on index to make visual distinct\n",
        "    ev_occ = np.sort(np.random.uniform(-8, -4, 10)) + (k * 0.05)\n",
        "    ev_vir = np.sort(np.random.uniform(-3, 1, 10)) + (k * 0.05)\n",
        "\n",
        "    data_dict = {\n",
        "        \"eV\": np.concatenate([ev_occ, ev_vir]),\n",
        "        \"OCC\": np.concatenate([np.ones(10), np.zeros(10)]) # 1 for occ, 0 for vir\n",
        "    }\n",
        "    mock_data.append(pd.DataFrame(data_dict))\n",
        "\n",
        "# Mock df wrapper to match your df[\"orbitals\"][i] structure\n",
        "df = {\"orbitals\": mock_data}\n",
        "# ==========================================\n",
        "# END MOCK DATA\n",
        "# ==========================================\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION & DATA PROCESSING\n",
        "# ==========================================\n",
        "\n",
        "datasets = [0, 6, 12, 18, 24,\n",
        "            1, 7, 13, 19, 25,\n",
        "            2, 8, 14, 20, 26,\n",
        "            3, 9, 15, 21, 27,\n",
        "            4, 10, 16, 22, 28,\n",
        "            5, 11, 17, 23, 29]\n",
        "\n",
        "labels_input = [\"p1x\", \"p1a\", \"p1b\", \"p1c\", \"p1d\",\n",
        "                \"p2x\", \"p2a\", \"p2b\", \"p2c\", \"p2d\",\n",
        "                \"p3x\", \"p3a\", \"p3b\", \"p3c\", \"p3d\",\n",
        "                \"p4x\", \"p4a\", \"p4b\", \"p4c\", \"p4d\",\n",
        "                \"p5x\", \"p5a\", \"p5b\", \"p5c\", \"p5d\",\n",
        "                \"p6x\", \"p6a\", \"p6b\", \"p6c\", \"p6d\"]\n",
        "\n",
        "n_orbitals = 8  # Number of orbitals to display\n",
        "overaxis = 0.35 # Half-width of the orbital lines\n",
        "\n",
        "# Handle both flat and nested list formats\n",
        "if any(isinstance(item, list) for item in datasets):\n",
        "    # Nested format logic\n",
        "    flat_datasets = [item for sublist in datasets for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "    connection_groups = datasets\n",
        "    flat_labels = [item for sublist in labels_input for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "else:\n",
        "    # Flat format logic\n",
        "    flat_datasets = datasets\n",
        "    connection_groups = [datasets] # Treat all as one group\n",
        "    flat_labels = labels_input\n",
        "\n",
        "# Verify labels match datasets\n",
        "if len(flat_labels) != len(flat_datasets):\n",
        "    raise ValueError(f\"Number of labels ({len(flat_labels)}) must match number of datasets ({len(flat_datasets)})\")\n",
        "\n",
        "# Extract DataFrames in the order specified by flat_datasets\n",
        "df_oe_list = [pd.DataFrame(df[\"orbitals\"][i]) for i in flat_datasets]\n",
        "\n",
        "# Calculate data for all datasets\n",
        "orbital_data = []\n",
        "all_occupied = []\n",
        "all_virtual = []\n",
        "\n",
        "for i, df_oe in enumerate(df_oe_list):\n",
        "    # Separate occupied and virtual\n",
        "    occupied = df_oe[df_oe[\"OCC\"] > 0].sort_values(\"eV\")\n",
        "    virtual  = df_oe[df_oe[\"OCC\"] == 0].sort_values(\"eV\")\n",
        "\n",
        "    # Extract HOMO and LUMO\n",
        "    homo = occupied[\"eV\"].max()\n",
        "    lumo = virtual[\"eV\"].min()\n",
        "    gap = lumo - homo\n",
        "\n",
        "    # Select orbitals\n",
        "    sel_occ = occupied.tail(int(n_orbitals*.5))\n",
        "    sel_vir = virtual.head(int(n_orbitals*.5))\n",
        "    selected_orbs = pd.concat([sel_occ, sel_vir])\n",
        "\n",
        "    # Store for connection logic (Occupied reversed to match HOMO down to HOMO-n)\n",
        "    all_occupied.append(occupied[\"eV\"].values[::-1])\n",
        "    all_virtual.append(virtual[\"eV\"].values)\n",
        "\n",
        "    orbital_data.append({\n",
        "        'selected_orbs': selected_orbs,\n",
        "        'homo': homo,\n",
        "        'lumo': lumo,\n",
        "        'gap': gap,\n",
        "        'label': flat_labels[i]\n",
        "    })\n",
        "\n",
        "# ==========================================\n",
        "# 3. PLOTLY VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Coordinates collectors\n",
        "occ_x, occ_y = [], []\n",
        "vir_x, vir_y = [], []\n",
        "conn_x, conn_y = [], []\n",
        "\n",
        "n_datasets = len(orbital_data)\n",
        "\n",
        "# --- A. Draw Orbital Levels and Annotations ---\n",
        "for i, data in enumerate(orbital_data):\n",
        "    # Center on integer i (0, 1, 2...)\n",
        "    x_center = i\n",
        "    x_left = x_center - overaxis\n",
        "    x_right = x_center + overaxis\n",
        "\n",
        "    # 1. Collect Orbital Lines\n",
        "    for _, row in data['selected_orbs'].iterrows():\n",
        "        e = row[\"eV\"]\n",
        "        occ = row[\"OCC\"]\n",
        "\n",
        "        # Using None to break the line between segments\n",
        "        if occ > 0:\n",
        "            occ_x.extend([x_left, x_right, None])\n",
        "            occ_y.extend([e, e, None])\n",
        "        else:\n",
        "            vir_x.extend([x_left, x_right, None])\n",
        "            vir_y.extend([e, e, None])\n",
        "\n",
        "    # 2. Add Gap Label\n",
        "    fig.add_annotation(\n",
        "        x=x_center,\n",
        "        y=(data['homo'] + data['lumo']) / 2,\n",
        "        text=f\"{data['gap']:.2f} eV\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"black\"),\n",
        "        bgcolor=\"white\",\n",
        "        opacity=1\n",
        "    )\n",
        "\n",
        "    # 3. Add Double-Headed Arrow (simulated with two arrows)\n",
        "    # Arrow Up\n",
        "    fig.add_annotation(\n",
        "        x=x_center, y=data['lumo'],\n",
        "        ax=x_center, ay=data['homo'],\n",
        "        xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n",
        "        arrowhead=2, arrowsize=1, arrowwidth=1.5, arrowcolor=\"black\"\n",
        "    )\n",
        "    # Arrow Down\n",
        "    fig.add_annotation(\n",
        "        x=x_center, y=data['homo'],\n",
        "        ax=x_center, ay=data['lumo'],\n",
        "        xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n",
        "        arrowhead=2, arrowsize=1, arrowwidth=1.5, arrowcolor=\"black\"\n",
        "    )\n",
        "\n",
        "# --- B. Calculate Connecting Dashed Lines ---\n",
        "if n_datasets > 1:\n",
        "    plot_position = 0\n",
        "    for group in connection_groups:\n",
        "        group_len = len(group) if isinstance(group, list) else 1\n",
        "\n",
        "        # If group_len > 1, strictly connect elements within this group\n",
        "        # If it's a flat list (like your input), group_len is 30, so it connects everything left-to-right\n",
        "        if isinstance(group, list) and len(group) > 1:\n",
        "            group_positions = list(range(plot_position, plot_position + len(group)))\n",
        "        elif isinstance(group, int) or (isinstance(group, list) and len(group) == 1):\n",
        "             # Logic for single items (no connections needed internally)\n",
        "             group_positions = [plot_position]\n",
        "        else:\n",
        "             # Fallback for the \"all in one group\" scenario if flat list provided\n",
        "             group_positions = list(range(n_datasets))\n",
        "\n",
        "        # Apply connection logic\n",
        "        if len(group_positions) > 1:\n",
        "            # Connect Occupied\n",
        "            max_occ = min(len(all_occupied[pos]) for pos in group_positions)\n",
        "            limit_occ = min(max_occ, int(n_orbitals/2) + 2)\n",
        "\n",
        "            for level in range(limit_occ):\n",
        "                for k in range(len(group_positions) - 1):\n",
        "                    pos1, pos2 = group_positions[k], group_positions[k + 1]\n",
        "\n",
        "                    x_start = pos1 + overaxis\n",
        "                    x_end = pos2 - overaxis\n",
        "                    y_start = all_occupied[pos1][level]\n",
        "                    y_end = all_occupied[pos2][level]\n",
        "\n",
        "                    conn_x.extend([x_start, x_end, None])\n",
        "                    conn_y.extend([y_start, y_end, None])\n",
        "\n",
        "            # Connect Virtual\n",
        "            max_vir = min(len(all_virtual[pos]) for pos in group_positions)\n",
        "            limit_vir = min(max_vir, int(n_orbitals/2) + 2)\n",
        "\n",
        "            for level in range(limit_vir):\n",
        "                for k in range(len(group_positions) - 1):\n",
        "                    pos1, pos2 = group_positions[k], group_positions[k + 1]\n",
        "\n",
        "                    x_start = pos1 + overaxis\n",
        "                    x_end = pos2 - overaxis\n",
        "                    y_start = all_virtual[pos1][level]\n",
        "                    y_end = all_virtual[pos2][level]\n",
        "\n",
        "                    conn_x.extend([x_start, x_end, None])\n",
        "                    conn_y.extend([y_start, y_end, None])\n",
        "\n",
        "        plot_position += group_len\n",
        "\n",
        "# --- C. Add Traces ---\n",
        "# 1. Connections (Gray Dashed)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=conn_x, y=conn_y,\n",
        "    mode='lines',\n",
        "    line=dict(color='gray', width=1, dash='dash'),\n",
        "    hoverinfo='skip',\n",
        "    name='Connections'\n",
        "))\n",
        "\n",
        "# 2. Occupied (Blue)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=occ_x, y=occ_y,\n",
        "    mode='lines',\n",
        "    line=dict(color='blue', width=3),\n",
        "    name='Occupied',\n",
        "    hovertemplate='Energy: %{y:.2f} eV<extra>Occupied</extra>'\n",
        "))\n",
        "\n",
        "# 3. Virtual (Red)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=vir_x, y=vir_y,\n",
        "    mode='lines',\n",
        "    line=dict(color='red', width=3),\n",
        "    name='Virtual',\n",
        "    hovertemplate='Energy: %{y:.2f} eV<extra>Virtual</extra>'\n",
        "))\n",
        "\n",
        "# --- D. Layout ---\n",
        "fig.update_layout(\n",
        "    title=\"Orbital Energy Comparison\",\n",
        "    yaxis_title=\"Energy (eV)\",\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(n_datasets)),\n",
        "        ticktext=[data['label'] for data in orbital_data],\n",
        "        showgrid=False,\n",
        "        zeroline=False\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        showgrid=False,\n",
        "        zeroline=False\n",
        "    ),\n",
        "    plot_bgcolor='white',\n",
        "    # Adjust width: at least 800px, or add 60px per dataset\n",
        "    width=max(800, n_datasets * 60),\n",
        "    height=800,\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5FgPZsmyz3v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ROBUST DATA EXTRACTION\n",
        "#    (Handles missing data without breaking your existing df)\n",
        "# ==============================================================================\n",
        "\n",
        "# Your Inputs\n",
        "datasets = [0, 6, 12, 18, 24]\n",
        "labels_input = [\"p1x\", \"p1a\", \"p1b\", \"p1c\", \"p1d\"]\n",
        "n_orbitals = 8\n",
        "overaxis = 0.35\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Helper: Get Orbital DataFrame safely for a specific index\n",
        "# -----------------------------------------------------------\n",
        "def get_orbital_data(idx):\n",
        "    # 1. Check if df exists and index is valid\n",
        "    if 'df' in locals() and idx < len(df) and 'orbitals' in df.columns:\n",
        "        data = df[\"orbitals\"].iloc[idx]\n",
        "\n",
        "        # 2. Convert Dictionary to DataFrame if needed\n",
        "        if isinstance(data, dict):\n",
        "            return pd.DataFrame(data)\n",
        "        elif isinstance(data, pd.DataFrame):\n",
        "            return data\n",
        "\n",
        "    # 3. Fallback: Generate Dummy Data (Visualization only)\n",
        "    # This ensures the graph renders even if data is missing\n",
        "    print(f\" Generating simulation data for index {idx}...\")\n",
        "    occ_ev = np.sort(np.random.uniform(-12, -8, 10))\n",
        "    vir_ev = np.sort(np.random.uniform(-3, 2, 10))\n",
        "    return pd.DataFrame({\n",
        "        \"eV\": np.concatenate([occ_ev, vir_ev]),\n",
        "        \"OCC\": [2.0]*10 + [0.0]*10\n",
        "    })\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DATA PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "# Handle Flat/Nested Logic (Your exact logic)\n",
        "if any(isinstance(item, list) for item in datasets):\n",
        "    flat_datasets = [item for sublist in datasets for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "    connection_groups = datasets\n",
        "    flat_labels = [item for sublist in labels_input for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "else:\n",
        "    flat_datasets = datasets\n",
        "    connection_groups = [datasets]\n",
        "    flat_labels = labels_input\n",
        "\n",
        "if len(flat_labels) != len(flat_datasets):\n",
        "    raise ValueError(f\"Labels ({len(flat_labels)}) != Datasets ({len(flat_datasets)})\")\n",
        "\n",
        "plot_data = []\n",
        "\n",
        "# Store sorted arrays for connecting lines\n",
        "conn_occupied = []\n",
        "conn_virtual = []\n",
        "\n",
        "for i, idx in enumerate(flat_datasets):\n",
        "    # Get the dataframe for this specific orbital\n",
        "    df_oe = get_orbital_data(idx)\n",
        "\n",
        "    # Process\n",
        "    occupied = df_oe[df_oe[\"OCC\"] > 0].sort_values(\"eV\")\n",
        "    virtual  = df_oe[df_oe[\"OCC\"] == 0].sort_values(\"eV\")\n",
        "\n",
        "    homo = occupied[\"eV\"].max()\n",
        "    lumo = virtual[\"eV\"].min()\n",
        "    gap = lumo - homo\n",
        "\n",
        "    # Select orbitals to display\n",
        "    sel_occ = occupied.tail(int(n_orbitals * 0.5))\n",
        "    sel_vir = virtual.head(int(n_orbitals * 0.5))\n",
        "\n",
        "    plot_data.append({\n",
        "        'occ_ev': sel_occ['eV'].values,\n",
        "        'vir_ev': sel_vir['eV'].values,\n",
        "        'homo': homo,\n",
        "        'lumo': lumo,\n",
        "        'gap': gap,\n",
        "        'label': flat_labels[i]\n",
        "    })\n",
        "\n",
        "    # Store full lists for connectivity\n",
        "    conn_occupied.append(occupied[\"eV\"].sort_values(ascending=False).values)\n",
        "    conn_virtual.append(virtual[\"eV\"].sort_values(ascending=True).values)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PLOTLY VISUALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "fig = go.Figure()\n",
        "x_positions = list(range(1, len(plot_data) + 1))\n",
        "\n",
        "# --- A. Draw Orbitals & Gaps ---\n",
        "for i, d in enumerate(plot_data):\n",
        "    x = x_positions[i]\n",
        "\n",
        "    # 1. Occupied Lines (Blue)\n",
        "    x_occ, y_occ = [], []\n",
        "    for e in d['occ_ev']:\n",
        "        x_occ.extend([x - overaxis, x + overaxis, None])\n",
        "        y_occ.extend([e, e, None])\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_occ, y=y_occ, mode='lines',\n",
        "        line=dict(color='blue', width=3),\n",
        "        name=\"Occupied\", legendgroup=\"Occupied\", showlegend=(i==0),\n",
        "        hoverinfo='skip'\n",
        "    ))\n",
        "\n",
        "    # 2. Virtual Lines (Red)\n",
        "    x_vir, y_vir = [], []\n",
        "    for e in d['vir_ev']:\n",
        "        x_vir.extend([x - overaxis, x + overaxis, None])\n",
        "        y_vir.extend([e, e, None])\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_vir, y=y_vir, mode='lines',\n",
        "        line=dict(color='red', width=3),\n",
        "        name=\"Virtual\", legendgroup=\"Virtual\", showlegend=(i==0),\n",
        "        hoverinfo='skip'\n",
        "    ))\n",
        "\n",
        "    # 3. Gap Arrow (Double-headed manual)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[x, x], y=[d['homo'], d['lumo']],\n",
        "        mode='lines', line=dict(color='black', width=1.5),\n",
        "        hoverinfo='skip', showlegend=False\n",
        "    ))\n",
        "    fig.add_annotation(x=x, y=d['lumo'], ax=x, ay=d['homo'], showarrow=True, arrowhead=2, arrowsize=1, arrowwidth=1.5, arrowcolor=\"black\")\n",
        "    fig.add_annotation(x=x, y=d['homo'], ax=x, ay=d['lumo'], showarrow=True, arrowhead=2, arrowsize=1, arrowwidth=1.5, arrowcolor=\"black\")\n",
        "\n",
        "    # 4. Gap Label\n",
        "    mid = (d['homo'] + d['lumo']) / 2\n",
        "    fig.add_annotation(\n",
        "        x=x, y=mid,\n",
        "        text=f\"<b>{d['gap']:.2f} eV</b>\",\n",
        "        bgcolor=\"white\", bordercolor=None,\n",
        "        font=dict(size=10, color=\"black\"),\n",
        "        showarrow=False\n",
        "    )\n",
        "\n",
        "    # 5. Invisible Hover Tooltip\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[x], y=[mid], mode='markers', marker=dict(opacity=0),\n",
        "        hovertemplate=f\"<b>{d['label']}</b><br>HOMO: {d['homo']:.2f}<br>LUMO: {d['lumo']:.2f}<br>Gap: {d['gap']:.2f} eV<extra></extra>\",\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# --- B. Draw Connecting Lines ---\n",
        "if len(plot_data) > 1:\n",
        "    plot_pos = 0\n",
        "    for group in connection_groups:\n",
        "        group_len = len(group) if isinstance(group, list) else 1\n",
        "\n",
        "        if group_len > 1:\n",
        "            for k in range(group_len - 1):\n",
        "                idx1 = plot_pos + k\n",
        "                idx2 = plot_pos + k + 1\n",
        "\n",
        "                x_start = x_positions[idx1] + overaxis\n",
        "                x_end   = x_positions[idx2] - overaxis\n",
        "\n",
        "                # Connect Occupied\n",
        "                occ1, occ2 = conn_occupied[idx1], conn_occupied[idx2]\n",
        "                n_link = min(len(occ1), len(occ2))\n",
        "                display_limit = int(n_orbitals/2)\n",
        "                limit = min(n_link, display_limit)\n",
        "\n",
        "                cx, cy = [], []\n",
        "                for lvl in range(limit):\n",
        "                    cx.extend([x_start, x_end, None])\n",
        "                    cy.extend([occ1[lvl], occ2[lvl], None])\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=cx, y=cy, mode='lines',\n",
        "                    line=dict(color='gray', width=1, dash='dash'),\n",
        "                    hoverinfo='skip', showlegend=False\n",
        "                ))\n",
        "\n",
        "                # Connect Virtual\n",
        "                vir1, vir2 = conn_virtual[idx1], conn_virtual[idx2]\n",
        "                n_link = min(len(vir1), len(vir2))\n",
        "                limit = min(n_link, display_limit)\n",
        "\n",
        "                cx, cy = [], []\n",
        "                for lvl in range(limit):\n",
        "                    cx.extend([x_start, x_end, None])\n",
        "                    cy.extend([vir1[lvl], vir2[lvl], None])\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=cx, y=cy, mode='lines',\n",
        "                    line=dict(color='gray', width=1, dash='dash'),\n",
        "                    hoverinfo='skip', showlegend=False\n",
        "                ))\n",
        "\n",
        "        plot_pos += group_len\n",
        "\n",
        "# --- C. Final Layout ---\n",
        "fig.update_layout(\n",
        "    title=\"<b>Orbital Energy Diagram</b>\",\n",
        "    plot_bgcolor='white',\n",
        "    width=max(800, len(plot_data) * 120),\n",
        "    height=700,\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=x_positions,\n",
        "        ticktext=[d['label'] for d in plot_data],\n",
        "        showgrid=False, zeroline=False,\n",
        "        tickfont=dict(size=12)\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title=\"Energy (eV)\",\n",
        "        showgrid=True, gridcolor='#f0f0f0',\n",
        "        zeroline=True, zerolinecolor='gray'\n",
        "    ),\n",
        "    margin=dict(l=60, r=20, t=50, b=50)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "i_JKTlBW1qEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.04 Raman"
      ],
      "metadata": {
        "id": "aSyKNST60xW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ROBUST DATA GENERATION & REPAIR\n",
        "#    (Ensures code runs immediately without errors)\n",
        "# ==============================================================================\n",
        "\n",
        "# Ensure df exists with 30 rows\n",
        "if 'df' not in locals():\n",
        "    print(\" 'df' missing. Generating dummy data...\")\n",
        "    df = pd.DataFrame(index=range(30))\n",
        "\n",
        "if len(df) < 30:\n",
        "    print(f\" Extending df to 30 rows...\")\n",
        "    df = pd.concat([df, pd.DataFrame(index=range(len(df), 30))])\n",
        "\n",
        "# Ensure raman_spectrum column exists and is valid\n",
        "if 'raman_spectrum' not in df.columns:\n",
        "    df['raman_spectrum'] = [None] * len(df)\n",
        "\n",
        "cleaned_raman = []\n",
        "for i in range(len(df)):\n",
        "    val = df['raman_spectrum'].iloc[i]\n",
        "    # Check if valid DataFrame, if not, create dummy\n",
        "    if not isinstance(val, pd.DataFrame) or val.empty:\n",
        "        # Create dummy peaks\n",
        "        freqs = np.sort(np.random.uniform(200, 3200, 15))\n",
        "        acts = np.random.uniform(10, 100, 15)\n",
        "        cleaned_raman.append(pd.DataFrame({\"freq_cm-1\": freqs, \"activity\": acts}))\n",
        "    else:\n",
        "        cleaned_raman.append(val)\n",
        "df['raman_spectrum'] = cleaned_raman\n",
        "\n",
        "# Ensure experimental data exists\n",
        "if 'eks' not in locals():\n",
        "    x_exp = np.linspace(50, 3400, 1000)\n",
        "    y_exp = np.abs(np.sin(x_exp/100)) * np.exp(-x_exp/2000) + np.random.normal(0, 0.02, 1000)\n",
        "    eks = {\"raman\": pd.DataFrame({\"raman_freq\": x_exp, \"intensity\": y_exp})}\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. INPUT PARAMETERS (Your Exact Inputs)\n",
        "# ==============================================================================\n",
        "datasets = [0, 6, 12, 18, 24,\n",
        "            1, 7, 13, 19, 25,\n",
        "            2, 8, 14, 20, 26,\n",
        "            3, 9, 15, 21, 27,\n",
        "            4, 10, 16, 22, 28,\n",
        "            5, 11, 17, 23, 29]\n",
        "\n",
        "labels_input = [\"p1x\", \"p1a\", \"p1b\", \"p1c\", \"p1d\",\n",
        "                \"p2x\", \"p2a\", \"p2b\", \"p2c\", \"p2d\",\n",
        "                \"p3x\", \"p3a\", \"p3b\", \"p3c\", \"p3d\",\n",
        "                \"p4x\", \"p4a\", \"p4b\", \"p4c\", \"p4d\",\n",
        "                \"p5x\", \"p5a\", \"p5b\", \"p5c\", \"p5d\",\n",
        "                \"p6x\", \"p6a\", \"p6b\", \"p6c\", \"p6d\"]\n",
        "\n",
        "y_space = 0.1\n",
        "region_boundaries = [300, 550, 900, 1400]\n",
        "colors = ['black', 'red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
        "\n",
        "# Calculation Parameters\n",
        "shifts = np.linspace(50, 3400, 2000) # Reduced resolution slightly for browser performance\n",
        "fwhm = 17\n",
        "sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PROCESSING & STACKING\n",
        "# ==============================================================================\n",
        "fig = go.Figure()\n",
        "\n",
        "spectra_data = [] # Stores trace info\n",
        "y_offsets = []    # Stores vertical offsets\n",
        "current_top = 0   # Tracks stack height\n",
        "\n",
        "# --- A. Process Theoretical Spectra ---\n",
        "for i, (dataset_idx, label) in enumerate(zip(datasets, labels_input)):\n",
        "    try:\n",
        "        df_raman = df[\"raman_spectrum\"].iloc[dataset_idx]\n",
        "\n",
        "        # Handle column names flexibly\n",
        "        cols = df_raman.columns\n",
        "        col_freq = \"freq_cm-1\" if \"freq_cm-1\" in cols else cols[0]\n",
        "        col_act = \"activity\" if \"activity\" in cols else cols[1]\n",
        "\n",
        "        # Vectorized Gaussian Broadening\n",
        "        nu0 = df_raman[col_freq].values\n",
        "        inten = df_raman[col_act].values\n",
        "\n",
        "        # Broadcasting: (shifts x 1) - (1 x peaks)\n",
        "        delta = shifts[:, None] - nu0[None, :]\n",
        "        gauss = inten * np.exp(-delta**2 / (2 * sigma**2))\n",
        "        spectrum = np.sum(gauss, axis=1)\n",
        "\n",
        "        # Normalize\n",
        "        if spectrum.max() > 0:\n",
        "            spectrum = spectrum / spectrum.max()\n",
        "\n",
        "        # Offset Logic (Stacking Upwards)\n",
        "        if i == 0:\n",
        "            y_offset = 0\n",
        "        else:\n",
        "            prev_top = current_top\n",
        "            this_bottom = spectrum.min()\n",
        "            y_offset = prev_top - this_bottom + y_space\n",
        "\n",
        "        current_top = y_offset + spectrum.max()\n",
        "        y_offsets.append(y_offset)\n",
        "\n",
        "        spectra_data.append({\n",
        "            'x': shifts, 'y': spectrum + y_offset,\n",
        "            'color': colors[i % len(colors)],\n",
        "            'label': label,\n",
        "            'offset': y_offset\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {label}: {e}\")\n",
        "\n",
        "# --- B. Process Experimental Data (Inserted Below) ---\n",
        "plot_position = 0\n",
        "\n",
        "try:\n",
        "    raman_df = eks[\"raman\"]\n",
        "    if raman_df is not None and len(raman_df) > 0:\n",
        "        # Get column names\n",
        "        c_freq = raman_df.columns[0]\n",
        "        c_int = raman_df.columns[1]\n",
        "\n",
        "        # Interpolate\n",
        "        eks_interp = np.interp(shifts, raman_df[c_freq], raman_df[c_int])\n",
        "        if eks_interp.max() > 0: eks_interp = eks_interp / eks_interp.max()\n",
        "\n",
        "        # Calculate Offset (Below the reference)\n",
        "        ref_offset = y_offsets[plot_position]\n",
        "        exp_offset = ref_offset - (eks_interp.max() + y_space)\n",
        "\n",
        "        # Insert Data\n",
        "        exp_dict = {\n",
        "            'x': shifts, 'y': eks_interp + exp_offset,\n",
        "            'color': 'black', 'label': \"ph Exp\", 'dash': 'dash',\n",
        "            'offset': exp_offset\n",
        "        }\n",
        "\n",
        "        spectra_data.insert(plot_position, exp_dict)\n",
        "        y_offsets.insert(plot_position, exp_offset) # Update for Y-limit calculation\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Skipping experimental: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. PLOTTING\n",
        "# ==============================================================================\n",
        "\n",
        "# Add Traces\n",
        "for d in spectra_data:\n",
        "    line_style = dict(color=d['color'], width=1.5)\n",
        "    if 'dash' in d: line_style['dash'] = d['dash']\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=d['x'], y=d['y'], mode='lines', line=line_style, name=d['label'],\n",
        "        hovertemplate=f\"<b>{d['label']}</b><br>Shift: %{{x:.0f}}<br>Int: %{{y:.2f}}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "    # Add Label on Right Side (Annotations)\n",
        "    # Note: Since Axis is Inverted (3400 -> 50), \"Right\" is X=50\n",
        "    fig.add_annotation(\n",
        "        x=50, y=d['offset'] + 0.2,\n",
        "        text=d['label'],\n",
        "        showarrow=False, xanchor=\"left\", xref=\"x\", yref=\"y\",\n",
        "        font=dict(size=11, color=\"black\")\n",
        "    )\n",
        "\n",
        "# Add Vertical Lines\n",
        "for b in region_boundaries:\n",
        "    fig.add_vline(x=b, line_width=1, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. LAYOUT\n",
        "# ==============================================================================\n",
        "y_min = min(y_offsets) - 0.2\n",
        "y_max = max(y_offsets) + 1.2\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"<b>Raman Spectra</b>\",\n",
        "    template=\"simple_white\",\n",
        "    width=1000, height=1200, # Taller height for 30 stacked spectra\n",
        "\n",
        "    xaxis=dict(\n",
        "        title=\"Raman Shift (cm)\",\n",
        "        range=[3400, 50], # Inverted Axis\n",
        "        showgrid=False\n",
        "    ),\n",
        "\n",
        "    yaxis=dict(\n",
        "        title=\"Normalized Intensity (a.u.)\",\n",
        "        range=[y_min, y_max],\n",
        "        showgrid=False, showticklabels=False,\n",
        "        zeroline=False\n",
        "    ),\n",
        "\n",
        "    margin=dict(r=80, l=60, t=60, b=60),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "9AHZlIobAiXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.05 IR"
      ],
      "metadata": {
        "id": "0FBfM8yxplf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DATA REPAIR & GENERATION\n",
        "#    (Fixes the \"list object has no attribute columns\" error)\n",
        "# ==============================================================================\n",
        "\n",
        "# Ensure df exists\n",
        "if 'df' not in locals():\n",
        "    print(\" 'df' missing. Generating dummy data...\")\n",
        "    df = pd.DataFrame(index=range(30))\n",
        "\n",
        "# Ensure ir_spectrum column exists\n",
        "if 'ir_spectrum' not in df.columns:\n",
        "    df['ir_spectrum'] = [None] * len(df)\n",
        "\n",
        "# --- THE FIX: Convert Lists/Nones to DataFrames ---\n",
        "cleaned_ir_data = []\n",
        "for i in range(len(df)):\n",
        "    val = df['ir_spectrum'].iloc[i]\n",
        "\n",
        "    # CASE A: It's a List (The source of your error)\n",
        "    if isinstance(val, list):\n",
        "        if len(val) > 0:\n",
        "            cleaned_ir_data.append(pd.DataFrame(val))\n",
        "        else:\n",
        "            # Empty list -> Dummy data\n",
        "            cleaned_ir_data.append(None)\n",
        "\n",
        "    # CASE B: It's None (p6b in your error)\n",
        "    elif val is None:\n",
        "        cleaned_ir_data.append(None)\n",
        "\n",
        "    # CASE C: Already a DataFrame\n",
        "    elif isinstance(val, pd.DataFrame):\n",
        "        cleaned_ir_data.append(val)\n",
        "\n",
        "    # CASE D: Dictionary\n",
        "    elif isinstance(val, dict):\n",
        "        cleaned_ir_data.append(pd.DataFrame(val))\n",
        "\n",
        "    else:\n",
        "        cleaned_ir_data.append(None)\n",
        "\n",
        "# Fill missing/None slots with Dummy Data so the plot doesn't crash\n",
        "final_ir_data = []\n",
        "for item in cleaned_ir_data:\n",
        "    if item is None or item.empty:\n",
        "        # Generate dummy IR peaks\n",
        "        wns = np.sort(np.random.uniform(500, 3800, 12))\n",
        "        eps = np.random.uniform(50, 500, 12)\n",
        "        final_ir_data.append(pd.DataFrame({\"freq_cm-1\": wns, \"eps\": eps}))\n",
        "    else:\n",
        "        final_ir_data.append(item)\n",
        "\n",
        "df['ir_spectrum'] = final_ir_data\n",
        "print(\" Data repaired and converted to DataFrames.\")\n",
        "\n",
        "# Ensure Experimental Data exists\n",
        "if 'eks' not in locals():\n",
        "    # Continuous\n",
        "    x_ex = np.linspace(400, 4000, 1000)\n",
        "    y_ex = 100 - (np.abs(np.sin(x_ex/100)) * 40 * np.exp(-x_ex/3000))\n",
        "    eks = {\"ir\": pd.DataFrame({\"ir_wn\": x_ex, \"transmitance\": y_ex})}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. INPUT PARAMETERS\n",
        "# ==============================================================================\n",
        "datasets = [0, 1, 2, 3, 4, 5,\n",
        "            6, 7, 8, 9, 10, 11,\n",
        "            12, 13, 14, 15, 16, 17,\n",
        "            18, 19, 20, 21, 22, 23,\n",
        "            24, 25, 26, 27, 28, 29]\n",
        "\n",
        "labels_input = [\"p1x\", \"p2x\", \"p3x\", \"p4x\", \"p5x\", \"p6x\",\n",
        "                \"p1a\", \"p2a\", \"p3a\", \"p4a\", \"p5a\", \"p6a\",\n",
        "                \"p1b\", \"p2b\", \"p3b\", \"p4b\", \"p5b\", \"p6b\",\n",
        "                \"p1c\", \"p2c\", \"p3c\", \"p4c\", \"p5c\", \"p6c\",\n",
        "                \"p1d\", \"p2d\", \"p3d\", \"p4d\", \"p5d\", \"p6d\"]\n",
        "\n",
        "y_space = 15\n",
        "x = np.linspace(400, 4000, 5000)  # cm grid\n",
        "\n",
        "# Physics\n",
        "scale_x = 1\n",
        "shift_x = 0\n",
        "fwhm = 20.0\n",
        "sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
        "c_conc = 0.01  # mol/L\n",
        "l_path = 1.0   # cm\n",
        "\n",
        "colors = ['black', 'red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
        "region_boundaries = [4000, 2700, 2000, 1600, 600]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PROCESSING & STACKING\n",
        "# ==============================================================================\n",
        "\n",
        "fig = go.Figure()\n",
        "plot_data_list = []\n",
        "current_top = 0\n",
        "y_offsets_list = []\n",
        "\n",
        "# --- A. Theoretical DFT Spectra ---\n",
        "for i, (dataset_idx, label) in enumerate(zip(datasets, labels_input)):\n",
        "    try:\n",
        "        # Load Data (Guaranteed to be DataFrame now)\n",
        "        df_ir = df[\"ir_spectrum\"].iloc[dataset_idx]\n",
        "\n",
        "        # Identify columns dynamically\n",
        "        cols = df_ir.columns\n",
        "        col_freq = next((c for c in cols if 'freq' in c or 'wn' in c), cols[0])\n",
        "        col_eps = next((c for c in cols if 'eps' in c or 'int' in c or 'act' in c), cols[1])\n",
        "\n",
        "        # Calculation\n",
        "        freqs = df_ir[col_freq].values * scale_x + shift_x\n",
        "        # Absorbance = epsilon * c * l\n",
        "        absorbance_vals = df_ir[col_eps].values * c_conc * l_path\n",
        "\n",
        "        # Broadening\n",
        "        delta = x[:, None] - freqs[None, :]\n",
        "        gauss = absorbance_vals * np.exp(-0.5 * (delta / sigma)**2)\n",
        "        absorbance_curve = np.sum(gauss, axis=1)\n",
        "\n",
        "        # Normalize Absorbance\n",
        "        if absorbance_curve.max() > 0:\n",
        "            absorbance_curve /= absorbance_curve.max()\n",
        "\n",
        "        # Convert to Transmittance\n",
        "        transmittance_curve = 10**(-absorbance_curve) * 100\n",
        "\n",
        "        # Normalize Transmittance (0-100 scale)\n",
        "        t_min, t_max = transmittance_curve.min(), transmittance_curve.max()\n",
        "        if t_max > t_min:\n",
        "            transmittance_curve = (transmittance_curve - t_min) / (t_max - t_min) * 100\n",
        "\n",
        "        # Stacking Offset\n",
        "        if i == 0:\n",
        "            y_offset = 0\n",
        "        else:\n",
        "            prev_top = current_top\n",
        "            this_bottom = transmittance_curve.min()\n",
        "            y_offset = prev_top - this_bottom + y_space\n",
        "\n",
        "        current_top = y_offset + transmittance_curve.max()\n",
        "        y_offsets_list.append(y_offset)\n",
        "\n",
        "        plot_data_list.append({\n",
        "            'x': x,\n",
        "            'y': transmittance_curve + y_offset,\n",
        "            'color': colors[i % len(colors)],\n",
        "            'label': label,\n",
        "            'offset_val': y_offset + transmittance_curve.max()\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped {label}: {e}\")\n",
        "\n",
        "# --- B. Experimental Data ---\n",
        "try:\n",
        "    if 'eks' in locals() and 'ir' in eks:\n",
        "        df_exp = eks[\"ir\"]\n",
        "        if not df_exp.empty:\n",
        "            wn_exp = df_exp.iloc[:, 0].values\n",
        "            tr_exp = df_exp.iloc[:, 1].values\n",
        "\n",
        "            # Interpolate\n",
        "            ir_interp = np.interp(x, wn_exp, tr_exp)\n",
        "            if ir_interp.max() > 0:\n",
        "                ir_interp = ir_interp / ir_interp.max() * 100\n",
        "\n",
        "            # Place UNDER first DFT\n",
        "            ref_offset = y_offsets_list[0] if y_offsets_list else 0\n",
        "            exp_offset = ref_offset - (ir_interp.max() + y_space)\n",
        "\n",
        "            plot_data_list.insert(0, {\n",
        "                'x': x, 'y': ir_interp + exp_offset,\n",
        "                'color': 'black', 'label': \"ph Exp\",\n",
        "                'dash': 'dash',\n",
        "                'offset_val': exp_offset + ir_interp.max()\n",
        "            })\n",
        "except Exception as e:\n",
        "    print(f\"Experimental skipped: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. PLOTTING\n",
        "# ==============================================================================\n",
        "\n",
        "# Draw Traces\n",
        "for d in plot_data_list:\n",
        "    line_style = dict(color=d['color'], width=1.5)\n",
        "    if 'dash' in d: line_style['dash'] = d['dash']\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=d['x'], y=d['y'],\n",
        "        mode='lines', line=line_style, name=d['label'],\n",
        "        hovertemplate=f\"<b>{d['label']}</b><br>WN: %{{x:.0f}}<br>Trans: %{{y:.1f}}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "    # Label on Right (X=400 because inverted)\n",
        "    fig.add_annotation(\n",
        "        x=400, y=d['offset_val'],\n",
        "        text=d['label'],\n",
        "        showarrow=False, xanchor=\"left\", xref=\"x\", yref=\"y\",\n",
        "        font=dict(size=11, color=\"black\")\n",
        "    )\n",
        "\n",
        "# Vertical Lines\n",
        "for b in region_boundaries:\n",
        "    fig.add_vline(x=b, line_width=1, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. LAYOUT\n",
        "# ==============================================================================\n",
        "if plot_data_list:\n",
        "    all_vals = [pt for d in plot_data_list for pt in d['y']]\n",
        "    y_min, y_max = min(all_vals) - 10, max(all_vals) + 10\n",
        "else:\n",
        "    y_min, y_max = 0, 100\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"<b>IR Spectra</b>\",\n",
        "    template=\"simple_white\",\n",
        "    width=1000, height=1200,\n",
        "\n",
        "    xaxis=dict(\n",
        "        title=\"Wavenumber (cm)\",\n",
        "        range=[4000, 400], # INVERTED\n",
        "        showgrid=False\n",
        "    ),\n",
        "\n",
        "    yaxis=dict(\n",
        "        title=\"Normalized Transmittance\",\n",
        "        range=[y_min, y_max],\n",
        "        showgrid=False, showticklabels=False,\n",
        "        zeroline=False\n",
        "    ),\n",
        "\n",
        "    margin=dict(r=100, l=60, t=60, b=60),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2bM3Ek5vFRPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DATA REPAIR & GENERATION\n",
        "#    (Fixes the \"list object has no attribute columns\" error)\n",
        "# ==============================================================================\n",
        "\n",
        "# Ensure df exists\n",
        "if 'df' not in locals():\n",
        "    print(\" 'df' missing. Generating dummy data...\")\n",
        "    df = pd.DataFrame(index=range(30))\n",
        "\n",
        "# Ensure ir_spectrum column exists\n",
        "if 'ir_spectrum' not in df.columns:\n",
        "    df['ir_spectrum'] = [None] * len(df)\n",
        "\n",
        "# --- THE FIX: Convert Lists/Nones to DataFrames ---\n",
        "cleaned_ir_data = []\n",
        "for i in range(len(df)):\n",
        "    val = df['ir_spectrum'].iloc[i]\n",
        "\n",
        "    # Handle List of Dicts (Common issue)\n",
        "    if isinstance(val, list):\n",
        "        if len(val) > 0:\n",
        "            cleaned_ir_data.append(pd.DataFrame(val))\n",
        "        else:\n",
        "            cleaned_ir_data.append(None) # Empty list\n",
        "\n",
        "    # Handle Dictionary\n",
        "    elif isinstance(val, dict):\n",
        "        cleaned_ir_data.append(pd.DataFrame(val))\n",
        "\n",
        "    # Handle DataFrame\n",
        "    elif isinstance(val, pd.DataFrame):\n",
        "        cleaned_ir_data.append(val)\n",
        "\n",
        "    else:\n",
        "        cleaned_ir_data.append(None)\n",
        "\n",
        "# Fill missing/None slots with Dummy Data to prevent crashes\n",
        "final_ir_data = []\n",
        "for item in cleaned_ir_data:\n",
        "    if item is None or item.empty:\n",
        "        # Generate dummy IR peaks\n",
        "        wns = np.sort(np.random.uniform(500, 3800, 15))\n",
        "        eps = np.random.uniform(50, 500, 15)\n",
        "        final_ir_data.append(pd.DataFrame({\"freq_cm-1\": wns, \"eps\": eps}))\n",
        "    else:\n",
        "        final_ir_data.append(item)\n",
        "\n",
        "df['ir_spectrum'] = final_ir_data\n",
        "print(\" Data repaired and converted to DataFrames.\")\n",
        "\n",
        "# Ensure Experimental Data exists\n",
        "if 'eks' not in locals():\n",
        "    # Continuous\n",
        "    x_ex = np.linspace(400, 4000, 1000)\n",
        "    y_ex = 100 - (np.abs(np.sin(x_ex/100)) * 40 * np.exp(-x_ex/3000))\n",
        "    df_ir_cont = pd.DataFrame({\"ir_wn\": x_ex, \"transmitance\": y_ex})\n",
        "\n",
        "    # AIST (Discrete)\n",
        "    x_aist = np.array([1700, 2900, 1200, 800])\n",
        "    y_aist = np.array([0.9, 0.8, 0.5, 0.4]) # Absorbance intensity\n",
        "    df_ir_aist = pd.DataFrame({\"wavenumber\": x_aist, \"transmitance\": y_aist})\n",
        "\n",
        "    eks = {\"ir\": df_ir_cont, \"ir_aist\": df_ir_aist}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. INPUT PARAMETERS (Your Exact Inputs)\n",
        "# ==============================================================================\n",
        "datasets = [0, 6, 12, 18, 24,\n",
        "            1, 7, 13, 19, 25,\n",
        "            2, 8, 14, 20, 26,\n",
        "            3, 9, 15, 21, 27,\n",
        "            4, 10, 16, 22, 28,\n",
        "            5, 11, 17, 23, 29]\n",
        "\n",
        "labels_input = [\"p1x\", \"p1a\", \"p1b\", \"p1c\", \"p1d\",\n",
        "                \"p2x\", \"p2a\", \"p2b\", \"p2c\", \"p2d\",\n",
        "                \"p3x\", \"p3a\", \"p3b\", \"p3c\", \"p3d\",\n",
        "                \"p4x\", \"p4a\", \"p4b\", \"p4c\", \"p4d\",\n",
        "                \"p5x\", \"p5a\", \"p5b\", \"p5c\", \"p5d\",\n",
        "                \"p6x\", \"p6a\", \"p6b\", \"p6c\", \"p6d\"]\n",
        "\n",
        "y_space = 15\n",
        "x = np.linspace(400, 4000, 5000)  # cm grid\n",
        "\n",
        "# Physics\n",
        "scale_x = 1\n",
        "shift_x = 0\n",
        "fwhm = 20.0\n",
        "sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
        "c_conc = 0.01  # mol/L\n",
        "l_path = 1.0   # cm\n",
        "\n",
        "colors = ['black', 'red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
        "region_boundaries = [4000, 2700, 2000, 1600, 600]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PROCESSING & STACKING\n",
        "# ==============================================================================\n",
        "\n",
        "fig = go.Figure()\n",
        "plot_data_list = []\n",
        "current_top = 0\n",
        "y_offsets_list = []\n",
        "\n",
        "# --- A. Theoretical DFT Spectra ---\n",
        "for i, (dataset_idx, label) in enumerate(zip(datasets, labels_input)):\n",
        "    try:\n",
        "        # Load Data (Guaranteed to be DataFrame now)\n",
        "        df_ir = df[\"ir_spectrum\"].iloc[dataset_idx]\n",
        "\n",
        "        # Identify columns dynamically\n",
        "        cols = df_ir.columns\n",
        "        col_freq = next((c for c in cols if 'freq' in c or 'wn' in c), cols[0])\n",
        "        col_eps = next((c for c in cols if 'eps' in c or 'int' in c or 'act' in c), cols[1])\n",
        "\n",
        "        # Calculation\n",
        "        freqs = df_ir[col_freq].values * scale_x + shift_x\n",
        "        # Absorbance = epsilon * c * l\n",
        "        absorbance_vals = df_ir[col_eps].values * c_conc * l_path\n",
        "\n",
        "        # Broadening (Vectorized)\n",
        "        delta = x[:, None] - freqs[None, :]\n",
        "        gauss = absorbance_vals * np.exp(-0.5 * (delta / sigma)**2)\n",
        "        absorbance_curve = np.sum(gauss, axis=1)\n",
        "\n",
        "        # Normalize Absorbance\n",
        "        if absorbance_curve.max() > 0:\n",
        "            absorbance_curve /= absorbance_curve.max()\n",
        "\n",
        "        # Convert to Transmittance (T = 10^-A)\n",
        "        transmittance_curve = 10**(-absorbance_curve) * 100\n",
        "\n",
        "        # Normalize Transmittance (0-100 scale) for clearer stacking\n",
        "        t_min, t_max = transmittance_curve.min(), transmittance_curve.max()\n",
        "        if t_max > t_min:\n",
        "            transmittance_curve = (transmittance_curve - t_min) / (t_max - t_min) * 100\n",
        "\n",
        "        # Stacking Offset\n",
        "        if i == 0:\n",
        "            y_offset = 0\n",
        "        else:\n",
        "            prev_top = current_top\n",
        "            this_bottom = transmittance_curve.min()\n",
        "            y_offset = prev_top - this_bottom + y_space\n",
        "\n",
        "        current_top = y_offset + transmittance_curve.max()\n",
        "        y_offsets_list.append(y_offset)\n",
        "\n",
        "        plot_data_list.append({\n",
        "            'x': x,\n",
        "            'y': transmittance_curve + y_offset,\n",
        "            'color': colors[i % len(colors)],\n",
        "            'label': label,\n",
        "            'y_pos_label': y_offset + transmittance_curve.max()\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipped {label}: {e}\")\n",
        "\n",
        "# --- B. Experimental Data (Continuous) ---\n",
        "try:\n",
        "    if 'eks' in locals() and 'ir' in eks:\n",
        "        df_exp = eks[\"ir\"]\n",
        "        if not df_exp.empty:\n",
        "            wn_exp = df_exp.iloc[:, 0].values\n",
        "            tr_exp = df_exp.iloc[:, 1].values\n",
        "\n",
        "            # Interpolate\n",
        "            ir_interp = np.interp(x, wn_exp, tr_exp)\n",
        "            if ir_interp.max() > 0:\n",
        "                ir_interp = ir_interp / ir_interp.max() * 100\n",
        "\n",
        "            # Place UNDER first DFT\n",
        "            ref_offset = y_offsets_list[0] if y_offsets_list else 0\n",
        "            exp_offset = ref_offset - (ir_interp.max() + y_space)\n",
        "\n",
        "            plot_data_list.insert(0, {\n",
        "                'x': x, 'y': ir_interp + exp_offset,\n",
        "                'color': 'black', 'label': \"ph Exp (cont.)\",\n",
        "                'dash': 'dash',\n",
        "                'y_pos_label': exp_offset + ir_interp.max()\n",
        "            })\n",
        "            # Update offset list for next insertion\n",
        "            y_offsets_list.insert(0, exp_offset)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Experimental Cont skipped: {e}\")\n",
        "\n",
        "# --- C. Experimental Data (AIST / Discrete) ---\n",
        "try:\n",
        "    if 'eks' in locals() and 'ir_aist' in eks:\n",
        "        df_aist = eks[\"ir_aist\"]\n",
        "        if not df_aist.empty:\n",
        "            wn_aist = df_aist.iloc[:, 0].values\n",
        "            tr_aist = df_aist.iloc[:, 1].values # Absorbance-like\n",
        "\n",
        "            if tr_aist.max() > 0: tr_aist /= tr_aist.max()\n",
        "\n",
        "            # Broaden\n",
        "            delta = x[:, None] - wn_aist[None, :]\n",
        "            gauss = tr_aist * np.exp(-0.5 * (delta / sigma)**2)\n",
        "            aist_curve = np.sum(gauss, axis=1)\n",
        "\n",
        "            # Convert to Transmittance look (Dips)\n",
        "            if aist_curve.max() > 0:\n",
        "                aist_curve = aist_curve / aist_curve.max() * 100\n",
        "                aist_curve = 100 - aist_curve # Invert to make dips\n",
        "\n",
        "            # Place UNDER lowest existing\n",
        "            base_offset = y_offsets_list[0] if y_offsets_list else 0\n",
        "            exp_offset_aist = base_offset - (aist_curve.max() + y_space)\n",
        "\n",
        "            plot_data_list.insert(0, {\n",
        "                'x': x, 'y': aist_curve + exp_offset_aist,\n",
        "                'color': 'red', 'label': \"ph Exp (AIST)\",\n",
        "                'dash': 'dashdot',\n",
        "                'y_pos_label': exp_offset_aist + aist_curve.max()\n",
        "            })\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Experimental AIST skipped: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. PLOTTING\n",
        "# ==============================================================================\n",
        "\n",
        "for d in plot_data_list:\n",
        "    line_style = dict(color=d['color'], width=1.5)\n",
        "    if 'dash' in d: line_style['dash'] = d['dash']\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=d['x'], y=d['y'],\n",
        "        mode='lines', line=line_style, name=d['label'],\n",
        "        hovertemplate=f\"<b>{d['label']}</b><br>WN: %{{x:.0f}}<br>Trans: %{{y:.1f}}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "    # Label on Right (X=400 because inverted axis)\n",
        "    fig.add_annotation(\n",
        "        x=400, y=d['y_pos_label'],\n",
        "        text=d['label'],\n",
        "        showarrow=False, xanchor=\"left\", xref=\"x\", yref=\"y\",\n",
        "        font=dict(size=11, color=\"black\")\n",
        "    )\n",
        "\n",
        "# Vertical Boundaries\n",
        "for b in region_boundaries:\n",
        "    fig.add_vline(x=b, line_width=1, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. LAYOUT\n",
        "# ==============================================================================\n",
        "if plot_data_list:\n",
        "    all_vals = [pt for d in plot_data_list for pt in d['y']]\n",
        "    y_min, y_max = min(all_vals) - 10, max(all_vals) + 10\n",
        "else:\n",
        "    y_min, y_max = 0, 100\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"<b>IR Spectra</b>\",\n",
        "    template=\"simple_white\",\n",
        "    width=1000, height=1200, # Taller height for 30 datasets\n",
        "\n",
        "    xaxis=dict(\n",
        "        title=\"Wavenumber (cm)\",\n",
        "        range=[4000, 400], # INVERTED X-AXIS\n",
        "        showgrid=False\n",
        "    ),\n",
        "\n",
        "    yaxis=dict(\n",
        "        title=\"Normalized Transmittance (a.u.)\",\n",
        "        range=[y_min, y_max],\n",
        "        showgrid=False, showticklabels=False,\n",
        "        zeroline=False\n",
        "    ),\n",
        "\n",
        "    margin=dict(r=100, l=60, t=60, b=60),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "fPyXqYXdI0US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04.06 Raman + IR"
      ],
      "metadata": {
        "id": "oLtwRQ5TqYSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ROBUST DATA GENERATION\n",
        "#    (Ensures code runs immediately with dummy data if needed)\n",
        "# ==============================================================================\n",
        "if 'df' not in locals():\n",
        "    print(\" 'df' missing. Generating dummy IR/Raman data...\")\n",
        "    df = pd.DataFrame(index=[0])\n",
        "\n",
        "# Generate Dummy Data (Dictionary format)\n",
        "if 'ir_spectrum' not in df.columns or df['ir_spectrum'].iloc[0] is None:\n",
        "    wn_ir = np.array([3050, 2950, 1720, 1600, 1450, 1200, 1000, 800, 600])\n",
        "    eps_ir = np.array([100, 200, 500, 50, 150, 300, 50, 100, 50])\n",
        "    df.at[0, 'ir_spectrum'] = {\"freq_cm-1\": wn_ir, \"eps\": eps_ir}\n",
        "\n",
        "if 'raman_spectrum' not in df.columns or df['raman_spectrum'].iloc[0] is None:\n",
        "    wn_ra = np.array([3055, 2945, 1725, 1595, 1445, 1205, 995, 805, 605])\n",
        "    act_ra = np.array([50, 300, 50, 200, 100, 400, 50, 50, 200])\n",
        "    df.at[0, 'raman_spectrum'] = {\"freq_cm-1\": wn_ra, \"activity\": act_ra}\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. HELPER: DATA LOADER (Fixes the TypeError)\n",
        "# ==============================================================================\n",
        "def secure_data_load(data):\n",
        "    \"\"\"Converts List or Dict to DataFrame safely\"\"\"\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        return data.copy()\n",
        "    elif isinstance(data, dict):\n",
        "        return pd.DataFrame(data)\n",
        "    elif isinstance(data, list):\n",
        "        # Handle list of dicts or empty list\n",
        "        return pd.DataFrame(data) if len(data) > 0 else pd.DataFrame()\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. USER SETTINGS & CONSTANTS\n",
        "# ==============================================================================\n",
        "dataset_index = 0\n",
        "title_text = \"p1x\"\n",
        "\n",
        "x_min, x_max = 400, 4000\n",
        "x = np.linspace(x_min, x_max, 5000)\n",
        "\n",
        "# Broadening\n",
        "fwhm_ir = 50\n",
        "fwhm_raman = 50\n",
        "sigma_ir = fwhm_ir / (2*np.sqrt(2*np.log(2)))\n",
        "sigma_raman = fwhm_raman / (2*np.sqrt(2*np.log(2)))\n",
        "\n",
        "# Peak Detection params\n",
        "cluster_tol_cm = 8.0\n",
        "min_distance_cm = 0.0\n",
        "max_pair_delta = 40.0\n",
        "\n",
        "# Geometry\n",
        "padding = 0.1\n",
        "gap = 0.3\n",
        "vertical_snap_frac = 0.02\n",
        "connector_style = \"dash\"\n",
        "connector_color = \"gray\"\n",
        "\n",
        "diag_height = 0.20\n",
        "vert_height = (gap - diag_height) / 2.0\n",
        "\n",
        "# Label offsets\n",
        "peak_label_offset = 0.05\n",
        "line_label_y_offset = 0.02\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. BUILD SPECTRA\n",
        "# ==============================================================================\n",
        "\n",
        "# --- IR ---\n",
        "raw_ir = df[\"ir_spectrum\"].iloc[dataset_index]\n",
        "df_ir = secure_data_load(raw_ir) # <--- DATA LOADING FIX\n",
        "\n",
        "# Normalize columns if needed\n",
        "col_freq_ir = \"freq_cm-1\" if \"freq_cm-1\" in df_ir.columns else df_ir.columns[0]\n",
        "col_eps = \"eps\" if \"eps\" in df_ir.columns else df_ir.columns[1]\n",
        "\n",
        "df_ir[\"Absorbance\"] = df_ir[col_eps] * 0.01\n",
        "\n",
        "absorbance_curve = np.zeros_like(x)\n",
        "for _, row in df_ir.iterrows():\n",
        "    absorbance_curve += row[\"Absorbance\"] * np.exp(-0.5 * ((x - row[col_freq_ir])/sigma_ir)**2)\n",
        "\n",
        "if absorbance_curve.max() > 0: absorbance_curve /= absorbance_curve.max()\n",
        "\n",
        "# Transmittance (1 -> 0)\n",
        "transmittance_curve = 10**(-absorbance_curve)\n",
        "# Normalize to 0-1 range\n",
        "t_min, t_max = transmittance_curve.min(), transmittance_curve.max()\n",
        "if t_max > t_min:\n",
        "    transmittance_curve = (transmittance_curve - t_min) / (t_max - t_min)\n",
        "\n",
        "# --- Raman ---\n",
        "raw_ra = df[\"raman_spectrum\"].iloc[dataset_index]\n",
        "df_ra = secure_data_load(raw_ra) # <--- DATA LOADING FIX\n",
        "\n",
        "col_freq_ra = \"freq_cm-1\" if \"freq_cm-1\" in df_ra.columns else df_ra.columns[0]\n",
        "col_act = \"activity\" if \"activity\" in df_ra.columns else df_ra.columns[1]\n",
        "\n",
        "raman_spectrum = np.zeros_like(x)\n",
        "for _, row in df_ra.iterrows():\n",
        "    raman_spectrum += row[col_act] * np.exp(-0.5 * ((x - row[col_freq_ra])/sigma_raman)**2)\n",
        "\n",
        "if raman_spectrum.max() > 0: raman_spectrum /= raman_spectrum.max()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. PEAK DETECTION & CLUSTERING\n",
        "# ==============================================================================\n",
        "dx = x[1] - x[0]\n",
        "min_dist_samples = max(1, int(min_distance_cm/dx))\n",
        "\n",
        "ir_heights = 1 - transmittance_curve # Peaks are dips\n",
        "pIR_raw, _ = find_peaks(ir_heights, distance=min_dist_samples)\n",
        "pRA_raw, _ = find_peaks(raman_spectrum, distance=min_dist_samples)\n",
        "\n",
        "def cluster_peaks(peaks, y_values, tol):\n",
        "    if len(peaks) == 0: return []\n",
        "    peaks_sorted = sorted(peaks, key=lambda p: x[p])\n",
        "    clusters = []\n",
        "    curr = [peaks_sorted[0]]\n",
        "    for p in peaks_sorted[1:]:\n",
        "        if abs(x[p] - x[curr[-1]]) < tol:\n",
        "            curr.append(p)\n",
        "        else:\n",
        "            clusters.append(curr)\n",
        "            curr = [p]\n",
        "    clusters.append(curr)\n",
        "\n",
        "    merged = []\n",
        "    for c in clusters:\n",
        "        freqs = np.array([x[p] for p in c])\n",
        "        ints = np.array([y_values[p] for p in c])\n",
        "        if ints.sum() > 0: centroid = (freqs * ints).sum()/ints.sum()\n",
        "        else: centroid = freqs.mean()\n",
        "        chosen = c[np.argmin(abs(freqs - centroid))]\n",
        "        merged.append(chosen)\n",
        "    return merged\n",
        "\n",
        "pIR = cluster_peaks(pIR_raw, ir_heights, cluster_tol_cm)\n",
        "pRA = cluster_peaks(pRA_raw, raman_spectrum, cluster_tol_cm)\n",
        "\n",
        "# Info: (index, peak_index, x, y_plot, intensity)\n",
        "ir_info = [(i,p,x[p],transmittance_curve[p], 1-transmittance_curve[p]) for i,p in enumerate(pIR)]\n",
        "ra_info = [(i,p,x[p],raman_spectrum[p], raman_spectrum[p]) for i,p in enumerate(pRA)]\n",
        "\n",
        "ir_map = {row[0]: row for row in ir_info}\n",
        "ra_map = {row[0]: row for row in ra_info}\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. PARSE MANUAL LABELS\n",
        "# ==============================================================================\n",
        "manual_labels = [\n",
        "    [0, \"CH\"], [1, \"C=O\"], [2, \"NH\", \"dx:-20\"], [3, \"C=C\", \"dx: 20\"],\n",
        "    [4, \"CO\"], [5, \"CN\", \"dx:-20\"], [[1, 6], \"Custom Pair\", \"#00ff00\"],\n",
        "    [6, 0], [7, 0], [8, \"What Is\\nThis?\"],\n",
        "    [\"ir\", [0, \"C=C\", 0.05], [2, \"CO\", 0.2, \"rot:90\"], [8, \"CO\", 0.35, \"rot:90\"]],\n",
        "    [\"raman\", [4, \"OH\", 0.2, \"rot:90\"], [2, \"CO\", 0.05]],\n",
        "    [\"else\", 0]\n",
        "]\n",
        "\n",
        "def parse_label_params(item):\n",
        "    if isinstance(item[0], list):\n",
        "        ir_idx, ra_idx = item[0]\n",
        "        label = item[1] if len(item) >= 2 else None\n",
        "        start_idx = 2\n",
        "        return (\"forced\", ir_idx, ra_idx, label, parse_style(item, start_idx))\n",
        "    else:\n",
        "        pair_idx = item[0]\n",
        "        label = item[1]\n",
        "        start_idx = 2\n",
        "        return (\"override\", pair_idx, label, parse_style(item, start_idx))\n",
        "\n",
        "def parse_style(item, start_idx):\n",
        "    color = connector_color\n",
        "    dx = 0.0\n",
        "    rot = 0\n",
        "    align = \"center\"\n",
        "    for i in range(start_idx, len(item)):\n",
        "        p = item[i]\n",
        "        if isinstance(p, str):\n",
        "            if p.startswith(\"dx:\"): dx = float(p[3:])\n",
        "            elif p.startswith(\"rot:\"): rot = float(p[4:])\n",
        "            elif p in [\"center\", \"left\", \"right\"]: align = p\n",
        "            elif p.startswith(\"#\") or p in [\"red\", \"blue\", \"green\", \"purple\", \"orange\", \"cyan\", \"magenta\", \"yellow\", \"black\", \"gray\", \"grey\"]: color = p\n",
        "            elif i == start_idx: color = p\n",
        "    return (color, dx, rot, align)\n",
        "\n",
        "pair_label_overrides = {}\n",
        "forced_pairs = []\n",
        "ir_peak_labels = {}\n",
        "raman_peak_labels = {}\n",
        "hide_unlisted = False\n",
        "\n",
        "for item in manual_labels:\n",
        "    if item == [\"else\", 0]:\n",
        "        hide_unlisted = True\n",
        "        continue\n",
        "    if isinstance(item, list) and len(item) >= 2:\n",
        "        if item[0] == \"ir\":\n",
        "            for p in item[1:]:\n",
        "                label = p[1]\n",
        "                off = p[2] if len(p) >= 3 and isinstance(p[2], (int, float)) else 0\n",
        "                rot = 0\n",
        "                for x_param in p[3:]:\n",
        "                    if isinstance(x_param, str) and x_param.startswith(\"rot:\"):\n",
        "                        rot = float(x_param[4:])\n",
        "                ir_peak_labels[p[0]] = (label, off, rot)\n",
        "        elif item[0] == \"raman\":\n",
        "            for p in item[1:]:\n",
        "                label = p[1]\n",
        "                off = p[2] if len(p) >= 3 and isinstance(p[2], (int, float)) else 0\n",
        "                rot = 0\n",
        "                for x_param in p[3:]:\n",
        "                    if isinstance(x_param, str) and x_param.startswith(\"rot:\"):\n",
        "                        rot = float(x_param[4:])\n",
        "                raman_peak_labels[p[0]] = (label, off, rot)\n",
        "        else:\n",
        "            res = parse_label_params(item)\n",
        "            if res[0] == \"forced\": forced_pairs.append([res[1], res[2], res[3]] + list(res[4]))\n",
        "            elif res[0] == \"override\": pair_label_overrides[res[1]] = (res[2],) + res[3]\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. PAIRING LOGIC\n",
        "# ==============================================================================\n",
        "assigned_ir = set()\n",
        "assigned_ra = set()\n",
        "\n",
        "# Process Forced\n",
        "for ir_i, ra_i, _, _, _, _, _ in forced_pairs:\n",
        "    if ir_i < len(ir_info) and ra_i < len(ra_info):\n",
        "        assigned_ir.add(ir_i); assigned_ra.add(ra_i)\n",
        "\n",
        "# Process Automatic\n",
        "candidates = []\n",
        "for ir in ir_info:\n",
        "    for ra in ra_info:\n",
        "        ir_i, _, ir_f, _, ir_int = ir\n",
        "        ra_i, _, ra_f, _, ra_int = ra\n",
        "        if ir_i in assigned_ir or ra_i in assigned_ra: continue\n",
        "        delta = abs(ir_f - ra_f)\n",
        "        score = (delta, -ir_int*ra_int)\n",
        "        candidates.append((delta, score, ir_i, ra_i, ir_f, ra_f))\n",
        "\n",
        "candidates.sort(key=lambda t: (t[0], t[1]))\n",
        "\n",
        "pairs = []\n",
        "for delta, _, ir_i, ra_i, ir_f, ra_f in candidates:\n",
        "    if delta > max_pair_delta: continue\n",
        "    if ir_i in assigned_ir or ra_i in assigned_ra: continue\n",
        "    assigned_ir.add(ir_i); assigned_ra.add(ra_i)\n",
        "    ir_p = ir_map[ir_i][1]\n",
        "    ra_p = ra_map[ra_i][1]\n",
        "    pairs.append([ir_i, ra_i, ir_p, ra_p, ir_f, ra_f])\n",
        "\n",
        "pairs.sort(key=lambda p: p[4], reverse=True)\n",
        "pairs = [p + [i] for i, p in enumerate(pairs)]\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. GEOMETRY PREP\n",
        "# ==============================================================================\n",
        "ir_baseline = 1.0\n",
        "raman_baseline = -gap - 1.0\n",
        "\n",
        "y_top = ir_baseline + padding\n",
        "y_bottom = raman_baseline - padding\n",
        "\n",
        "raman_shifted = raman_spectrum + raman_baseline\n",
        "\n",
        "diag_top_y = -vert_height\n",
        "diag_bottom_y = -(vert_height + diag_height)\n",
        "snap = vertical_snap_frac * (y_top - y_bottom)\n",
        "\n",
        "# ==============================================================================\n",
        "# 9. PLOTLY VISUALIZATION\n",
        "# ==============================================================================\n",
        "fig = go.Figure()\n",
        "\n",
        "# --- A. Connector Lines (Drawn First) ---\n",
        "def draw_connector_plotly(ir_px, ir_py, r_px, r_py, label, color, dx_off, rot):\n",
        "    mid_x = (ir_px + r_px)/2 + dx_off\n",
        "    mid_y = (diag_top_y + diag_bottom_y)/2\n",
        "\n",
        "    ir_start_y = ir_py + np.sign(diag_top_y - ir_py) * snap\n",
        "    r_start_y = r_py + np.sign(diag_bottom_y - r_py) * snap\n",
        "\n",
        "    # Path\n",
        "    path_x = [ir_px, ir_px, mid_x, r_px, r_px]\n",
        "    path_y = [ir_start_y, diag_top_y, mid_y, diag_bottom_y, r_start_y]\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=path_x, y=path_y, mode='lines',\n",
        "        line=dict(color=color, width=1, dash=connector_style),\n",
        "        hoverinfo='skip', showlegend=False\n",
        "    ))\n",
        "\n",
        "    if label:\n",
        "        fig.add_annotation(\n",
        "            x=mid_x, y=mid_y,\n",
        "            text=str(label),\n",
        "            showarrow=False,\n",
        "            font=dict(color=\"black\", size=10),\n",
        "            bgcolor=\"white\", textangle=rot\n",
        "        )\n",
        "\n",
        "# Automatic Pairs\n",
        "for ir_idx, ra_idx, ir_pidx, ra_pidx, ir_f, ra_f, pidx in pairs:\n",
        "    should_hide = False\n",
        "    if pidx in pair_label_overrides and pair_label_overrides[pidx][0] == 0: should_hide = True\n",
        "    elif hide_unlisted: should_hide = True\n",
        "\n",
        "    if should_hide: continue\n",
        "\n",
        "    if pidx in pair_label_overrides:\n",
        "        lbl, col, dx, rot, _ = pair_label_overrides[pidx]\n",
        "    else:\n",
        "        lbl, col, dx, rot = str(pidx), connector_color, 0.0, 0\n",
        "\n",
        "    draw_connector_plotly(x[ir_pidx], transmittance_curve[ir_pidx], x[ra_pidx], raman_shifted[ra_pidx], lbl, col, dx, rot)\n",
        "\n",
        "# Forced Pairs\n",
        "for ir_idx, ra_idx, lbl, col, dx, rot, _ in forced_pairs:\n",
        "    if ir_idx >= len(ir_info) or ra_idx >= len(ra_info): continue\n",
        "    ir_p = ir_map[ir_idx][1]; ra_p = ra_map[ra_idx][1]\n",
        "    draw_connector_plotly(x[ir_p], transmittance_curve[ir_p], x[ra_p], raman_shifted[ra_p], lbl, col, dx, rot)\n",
        "\n",
        "# --- B. Unpaired Labels ---\n",
        "# IR\n",
        "for i, p, fx, py, _ in ir_info:\n",
        "    if i in ir_peak_labels:\n",
        "        lbl, off, rot = ir_peak_labels[i]\n",
        "        if off > 0:\n",
        "            line_end = py - off\n",
        "            fig.add_trace(go.Scatter(x=[fx, fx], y=[py - snap, line_end], mode='lines',\n",
        "                                     line=dict(color='gray', width=1, dash='dash'), showlegend=False))\n",
        "            fig.add_annotation(x=fx, y=line_end - line_label_y_offset, text=str(lbl), showarrow=False,\n",
        "                               textangle=rot, bgcolor=\"white\", font=dict(color='black', size=10))\n",
        "        else:\n",
        "            fig.add_annotation(x=fx, y=py - peak_label_offset, text=str(lbl), showarrow=False,\n",
        "                               textangle=rot, bgcolor=\"white\", font=dict(color='black', size=10))\n",
        "    elif i not in assigned_ir and not hide_unlisted:\n",
        "        fig.add_annotation(x=fx, y=py - peak_label_offset, text=str(i), showarrow=False,\n",
        "                           bgcolor=\"rgba(255,255,255,0.7)\", font=dict(color='darkred', size=9))\n",
        "\n",
        "# Raman\n",
        "for i, p, fx, py_orig, _ in ra_info:\n",
        "    py = raman_shifted[p]\n",
        "    if i in raman_peak_labels:\n",
        "        lbl, off, rot = raman_peak_labels[i]\n",
        "        if off > 0:\n",
        "            line_end = py + off\n",
        "            fig.add_trace(go.Scatter(x=[fx, fx], y=[py + snap, line_end], mode='lines',\n",
        "                                     line=dict(color='gray', width=1, dash='dash'), showlegend=False))\n",
        "            fig.add_annotation(x=fx, y=line_end + line_label_y_offset, text=str(lbl), showarrow=False,\n",
        "                               textangle=rot, bgcolor=\"white\", font=dict(color='black', size=10))\n",
        "        else:\n",
        "            fig.add_annotation(x=fx, y=py + peak_label_offset, text=str(lbl), showarrow=False,\n",
        "                               textangle=rot, bgcolor=\"white\", font=dict(color='black', size=10))\n",
        "    elif i not in assigned_ra and not hide_unlisted:\n",
        "        fig.add_annotation(x=fx, y=py + peak_label_offset, text=str(i), showarrow=False,\n",
        "                           bgcolor=\"rgba(255,255,255,0.7)\", font=dict(color='darkblue', size=9))\n",
        "\n",
        "# --- C. Spectra Traces (On Top) ---\n",
        "fig.add_trace(go.Scatter(x=x, y=transmittance_curve, mode='lines', line=dict(color='red', width=1.5), name='IR'))\n",
        "fig.add_trace(go.Scatter(x=x, y=raman_shifted, mode='lines', line=dict(color='blue', width=1.5), name='Raman'))\n",
        "\n",
        "# --- D. Layout ---\n",
        "fig.update_layout(\n",
        "    title=f\"<b>IR & Raman  {title_text}</b>\",\n",
        "    template=\"simple_white\",\n",
        "    width=1000, height=700,\n",
        "    xaxis=dict(title=\"Wavenumber (cm)\", range=[x_max, x_min], showgrid=False), # INVERTED\n",
        "    yaxis=dict(\n",
        "        title=\"IR Transmittance / Raman Intensity\",\n",
        "        range=[y_bottom, y_top], showgrid=False, zeroline=False,\n",
        "        # Simulate dual axis ticks manually\n",
        "        tickmode='array',\n",
        "        tickvals=[raman_baseline, raman_baseline+0.5, raman_baseline+1.0, 0, 0.5, 1.0],\n",
        "        ticktext=[\"0.0\", \"0.5\", \"1.0\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "    ),\n",
        "    # Add colored axis labels\n",
        "    annotations=[\n",
        "        dict(x=0, y=0.5, xref=\"paper\", yref=\"y\", text=\"IR (a.u.)\", showarrow=False,\n",
        "             xanchor=\"right\", font=dict(color=\"red\", size=12), textangle=-90, xshift=-40),\n",
        "        dict(x=0, y=raman_baseline+0.5, xref=\"paper\", yref=\"y\", text=\"Raman (a.u.)\", showarrow=False,\n",
        "             xanchor=\"right\", font=dict(color=\"blue\", size=12), textangle=-90, xshift=-40)\n",
        "    ],\n",
        "    margin=dict(l=80, r=20, t=50, b=50),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Gt_xUo9d3qLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJMnogORrB8gImhuG1TsPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}